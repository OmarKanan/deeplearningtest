{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/omar/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "from time import time\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.metrics import f1_score, make_scorer, confusion_matrix, \\\n",
    "    classification_report\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, \\\n",
    "    StratifiedShuffleSplit, RandomizedSearchCV, GridSearchCV, StratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((28934, 300), (32151,), (32151,), (13779,), (13779,))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"../Data/Learn/labels.pkl\", \"rb\") as f:\n",
    "    learn_labels = pickle.load(f)\n",
    "\n",
    "with open(\"../Data/generated/my_learn_sequences.pkl\", \"rb\") as f:\n",
    "    learn_sequences = pickle.load(f)\n",
    "\n",
    "with open(\"../Data/generated/my_embeddings.pkl\", \"rb\") as f:\n",
    "    embeddings = pickle.load(f)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    learn_sequences, learn_labels, test_size=0.3,\n",
    "    shuffle=True, stratify=learn_labels, random_state=42 + 2\n",
    ")\n",
    "X_train, X_test = np.array(X_train), np.array(X_test)\n",
    "y_train, y_test = np.array(y_train), np.array(y_test)\n",
    "\n",
    "embeddings.shape, X_train.shape, y_train.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning rate and number of epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "\n",
    "n_splits, num_epochs = 3, 20\n",
    "learning_rates = [0.001, 0.005, 0.0075, 0.01, 0.025, 0.05, 0.075, 0.1]\n",
    "splitter = StratifiedKFold(n_splits, shuffle=True, random_state=1)\n",
    "results = {lr: {epoch: [] for epoch in range(num_epochs)} for lr in learning_rates}\n",
    "\n",
    "for lr in learning_rates:\n",
    "    for train_ind, test_ind in splitter.split(X_train, y_train):\n",
    "        model = LSTMModel(\n",
    "            weight_class_M=Counter(y_train)[\"C\"] / Counter(y_train)[\"M\"],\n",
    "            sentence_length=max(map(len, X_train)),\n",
    "            embeddings=embeddings,\n",
    "            num_units=50,\n",
    "            batch_size=128, \n",
    "            dropout_keep_prob=1.0,\n",
    "            learning_rate=lr,\n",
    "        )\n",
    "        for epoch in range(num_epochs):\n",
    "            with warnings.catch_warnings():\n",
    "                warnings.filterwarnings(\"ignore\")\n",
    "                model.fit(X_train[train_ind], y_train[train_ind])\n",
    "                score = model.score(X_train[test_ind], y_train[test_ind])\n",
    "                results[lr][epoch].append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0.001</th>\n",
       "      <th>0.005</th>\n",
       "      <th>0.0075</th>\n",
       "      <th>0.01</th>\n",
       "      <th>0.025</th>\n",
       "      <th>0.05</th>\n",
       "      <th>0.075</th>\n",
       "      <th>0.1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.219186</td>\n",
       "      <td>0.242043</td>\n",
       "      <td>0.257886</td>\n",
       "      <td>0.260620</td>\n",
       "      <td>0.281237</td>\n",
       "      <td>0.219403</td>\n",
       "      <td>0.113782</td>\n",
       "      <td>0.251390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.219518</td>\n",
       "      <td>0.273932</td>\n",
       "      <td>0.274697</td>\n",
       "      <td>0.292848</td>\n",
       "      <td>0.243436</td>\n",
       "      <td>0.221913</td>\n",
       "      <td>0.103890</td>\n",
       "      <td>0.224622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.219404</td>\n",
       "      <td>0.268741</td>\n",
       "      <td>0.341370</td>\n",
       "      <td>0.300429</td>\n",
       "      <td>0.236646</td>\n",
       "      <td>0.232590</td>\n",
       "      <td>0.190903</td>\n",
       "      <td>0.235841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.224053</td>\n",
       "      <td>0.299494</td>\n",
       "      <td>0.315628</td>\n",
       "      <td>0.294848</td>\n",
       "      <td>0.238888</td>\n",
       "      <td>0.217161</td>\n",
       "      <td>0.188953</td>\n",
       "      <td>0.202520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.226083</td>\n",
       "      <td>0.279089</td>\n",
       "      <td>0.357015</td>\n",
       "      <td>0.349260</td>\n",
       "      <td>0.221479</td>\n",
       "      <td>0.215590</td>\n",
       "      <td>0.196997</td>\n",
       "      <td>0.160555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.233993</td>\n",
       "      <td>0.268328</td>\n",
       "      <td>0.354776</td>\n",
       "      <td>0.323535</td>\n",
       "      <td>0.214209</td>\n",
       "      <td>0.229312</td>\n",
       "      <td>0.198591</td>\n",
       "      <td>0.196273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.240764</td>\n",
       "      <td>0.278549</td>\n",
       "      <td>0.361943</td>\n",
       "      <td>0.369923</td>\n",
       "      <td>0.225692</td>\n",
       "      <td>0.237473</td>\n",
       "      <td>0.198712</td>\n",
       "      <td>0.219391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.244185</td>\n",
       "      <td>0.282142</td>\n",
       "      <td>0.384298</td>\n",
       "      <td>0.383922</td>\n",
       "      <td>0.235038</td>\n",
       "      <td>0.222319</td>\n",
       "      <td>0.198420</td>\n",
       "      <td>0.223872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.250533</td>\n",
       "      <td>0.289522</td>\n",
       "      <td>0.398939</td>\n",
       "      <td>0.414784</td>\n",
       "      <td>0.155839</td>\n",
       "      <td>0.229974</td>\n",
       "      <td>0.198287</td>\n",
       "      <td>0.233108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.253443</td>\n",
       "      <td>0.295879</td>\n",
       "      <td>0.218489</td>\n",
       "      <td>0.401989</td>\n",
       "      <td>0.184943</td>\n",
       "      <td>0.227743</td>\n",
       "      <td>0.144751</td>\n",
       "      <td>0.236398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.255950</td>\n",
       "      <td>0.304276</td>\n",
       "      <td>0.372156</td>\n",
       "      <td>0.447079</td>\n",
       "      <td>0.224410</td>\n",
       "      <td>0.219213</td>\n",
       "      <td>0.199329</td>\n",
       "      <td>0.127484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.258398</td>\n",
       "      <td>0.303591</td>\n",
       "      <td>0.361880</td>\n",
       "      <td>0.434155</td>\n",
       "      <td>0.213444</td>\n",
       "      <td>0.217983</td>\n",
       "      <td>0.198678</td>\n",
       "      <td>0.144087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.261148</td>\n",
       "      <td>0.072515</td>\n",
       "      <td>0.361601</td>\n",
       "      <td>0.299552</td>\n",
       "      <td>0.215301</td>\n",
       "      <td>0.229155</td>\n",
       "      <td>0.198722</td>\n",
       "      <td>0.107216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.263378</td>\n",
       "      <td>0.143958</td>\n",
       "      <td>0.372765</td>\n",
       "      <td>0.125452</td>\n",
       "      <td>0.144661</td>\n",
       "      <td>0.220839</td>\n",
       "      <td>0.199171</td>\n",
       "      <td>0.127194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.264199</td>\n",
       "      <td>0.239989</td>\n",
       "      <td>0.372590</td>\n",
       "      <td>0.139452</td>\n",
       "      <td>0.213683</td>\n",
       "      <td>0.229225</td>\n",
       "      <td>0.199127</td>\n",
       "      <td>0.147565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.266496</td>\n",
       "      <td>0.281574</td>\n",
       "      <td>0.379425</td>\n",
       "      <td>0.174752</td>\n",
       "      <td>0.220044</td>\n",
       "      <td>0.221800</td>\n",
       "      <td>0.199017</td>\n",
       "      <td>0.149597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.268576</td>\n",
       "      <td>0.289433</td>\n",
       "      <td>0.379275</td>\n",
       "      <td>0.207233</td>\n",
       "      <td>0.219897</td>\n",
       "      <td>0.217547</td>\n",
       "      <td>0.198371</td>\n",
       "      <td>0.157852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.269369</td>\n",
       "      <td>0.289944</td>\n",
       "      <td>0.367696</td>\n",
       "      <td>0.216595</td>\n",
       "      <td>0.225123</td>\n",
       "      <td>0.224400</td>\n",
       "      <td>0.199021</td>\n",
       "      <td>0.153813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.270185</td>\n",
       "      <td>0.296075</td>\n",
       "      <td>0.377640</td>\n",
       "      <td>0.026337</td>\n",
       "      <td>0.227823</td>\n",
       "      <td>0.225875</td>\n",
       "      <td>0.197755</td>\n",
       "      <td>0.154679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.272641</td>\n",
       "      <td>0.298898</td>\n",
       "      <td>0.377015</td>\n",
       "      <td>0.097672</td>\n",
       "      <td>0.224985</td>\n",
       "      <td>0.221057</td>\n",
       "      <td>0.198686</td>\n",
       "      <td>0.138131</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      0.0010    0.0050    0.0075    0.0100    0.0250    0.0500    0.0750  \\\n",
       "0   0.219186  0.242043  0.257886  0.260620  0.281237  0.219403  0.113782   \n",
       "1   0.219518  0.273932  0.274697  0.292848  0.243436  0.221913  0.103890   \n",
       "2   0.219404  0.268741  0.341370  0.300429  0.236646  0.232590  0.190903   \n",
       "3   0.224053  0.299494  0.315628  0.294848  0.238888  0.217161  0.188953   \n",
       "4   0.226083  0.279089  0.357015  0.349260  0.221479  0.215590  0.196997   \n",
       "5   0.233993  0.268328  0.354776  0.323535  0.214209  0.229312  0.198591   \n",
       "6   0.240764  0.278549  0.361943  0.369923  0.225692  0.237473  0.198712   \n",
       "7   0.244185  0.282142  0.384298  0.383922  0.235038  0.222319  0.198420   \n",
       "8   0.250533  0.289522  0.398939  0.414784  0.155839  0.229974  0.198287   \n",
       "9   0.253443  0.295879  0.218489  0.401989  0.184943  0.227743  0.144751   \n",
       "10  0.255950  0.304276  0.372156  0.447079  0.224410  0.219213  0.199329   \n",
       "11  0.258398  0.303591  0.361880  0.434155  0.213444  0.217983  0.198678   \n",
       "12  0.261148  0.072515  0.361601  0.299552  0.215301  0.229155  0.198722   \n",
       "13  0.263378  0.143958  0.372765  0.125452  0.144661  0.220839  0.199171   \n",
       "14  0.264199  0.239989  0.372590  0.139452  0.213683  0.229225  0.199127   \n",
       "15  0.266496  0.281574  0.379425  0.174752  0.220044  0.221800  0.199017   \n",
       "16  0.268576  0.289433  0.379275  0.207233  0.219897  0.217547  0.198371   \n",
       "17  0.269369  0.289944  0.367696  0.216595  0.225123  0.224400  0.199021   \n",
       "18  0.270185  0.296075  0.377640  0.026337  0.227823  0.225875  0.197755   \n",
       "19  0.272641  0.298898  0.377015  0.097672  0.224985  0.221057  0.198686   \n",
       "\n",
       "      0.1000  \n",
       "0   0.251390  \n",
       "1   0.224622  \n",
       "2   0.235841  \n",
       "3   0.202520  \n",
       "4   0.160555  \n",
       "5   0.196273  \n",
       "6   0.219391  \n",
       "7   0.223872  \n",
       "8   0.233108  \n",
       "9   0.236398  \n",
       "10  0.127484  \n",
       "11  0.144087  \n",
       "12  0.107216  \n",
       "13  0.127194  \n",
       "14  0.147565  \n",
       "15  0.149597  \n",
       "16  0.157852  \n",
       "17  0.153813  \n",
       "18  0.154679  \n",
       "19  0.138131  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(results).applymap(np.mean) - pd.DataFrame(results).applymap(np.std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best result:\n",
    "- learning_rate = 0.01\n",
    "- num_epochs = 11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/omar/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:112: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           C       0.95      0.77      0.85     11974\n",
      "           M       0.32      0.71      0.44      1805\n",
      "\n",
      "   micro avg       0.76      0.76      0.76     13779\n",
      "   macro avg       0.63      0.74      0.64     13779\n",
      "weighted avg       0.86      0.76      0.79     13779\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "\n",
    "model = LSTMModel(\n",
    "    weight_class_M=Counter(y_train)[\"C\"] / Counter(y_train)[\"M\"],\n",
    "    sentence_length=max(map(len, X_train)),\n",
    "    embeddings=embeddings,\n",
    "    num_units=50,\n",
    "    batch_size=128, \n",
    "    dropout_keep_prob=1.0,\n",
    "    learning_rate=0.01,\n",
    ")\n",
    "\n",
    "model.fit(X_train, y_train, num_epochs=11)\n",
    "print(classification_report(y_test, model.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
