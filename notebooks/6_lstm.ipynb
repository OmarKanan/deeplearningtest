{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/omar/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "from time import time\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.metrics import f1_score, make_scorer, confusion_matrix, \\\n",
    "    classification_report\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, \\\n",
    "    StratifiedShuffleSplit, RandomizedSearchCV, GridSearchCV, StratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((28934, 300), (32151,), (32151,), (13779,), (13779,))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"../Data/Learn/labels.pkl\", \"rb\") as f:\n",
    "    learn_labels = pickle.load(f)\n",
    "\n",
    "with open(\"../Data/generated/my_learn_sequences.pkl\", \"rb\") as f:\n",
    "    learn_sequences = pickle.load(f)\n",
    "\n",
    "with open(\"../Data/generated/my_embeddings.pkl\", \"rb\") as f:\n",
    "    embeddings = pickle.load(f)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    learn_sequences, learn_labels, test_size=0.3,\n",
    "    shuffle=True, stratify=learn_labels, random_state=42 + 2\n",
    ")\n",
    "X_train, X_test = np.array(X_train), np.array(X_test)\n",
    "y_train, y_test = np.array(y_train), np.array(y_test)\n",
    "\n",
    "embeddings.shape, X_train.shape, y_train.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self,\n",
    "                 sentence_length,\n",
    "                 embeddings,\n",
    "                 num_units=50,\n",
    "                 batch_size=128,\n",
    "                 learning_rate=0.05,\n",
    "                 dropout_keep_prob=1.0,\n",
    "                 weight_class_M=1.0,\n",
    "                 model_name=None,\n",
    "                 checkpoints_dir=\"../checkpoints/\",\n",
    "                 ):\n",
    "        self.sentence_length = sentence_length\n",
    "        self.embeddings = embeddings\n",
    "        self.embedding_dim = self.embeddings.shape[1]\n",
    "        self.num_units = num_units\n",
    "        self.batch_size = batch_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.dropout_keep_prob = dropout_keep_prob\n",
    "        self.weight_class_M = weight_class_M\n",
    "        self.features_key = \"x\"\n",
    "        self.seq_lengths_key = \"lengths\"\n",
    "        self.model_dir = self.set_model_directory(checkpoints_dir, model_name)\n",
    "\n",
    "    @staticmethod\n",
    "    def set_model_directory(checkpoints_dir, model_name):\n",
    "        if model_name is not None:\n",
    "            model_dir = checkpoints_dir + model_name\n",
    "            # Check model_dir doesn't already exist\n",
    "            if os.path.exists(model_dir):\n",
    "                raise ValueError(\"model_dir already exists\")\n",
    "        else:\n",
    "            model_dir = None\n",
    "        return model_dir\n",
    "\n",
    "    def check_warm_start(self, warm_start):\n",
    "        if warm_start:\n",
    "            # Check if model was already fitted\n",
    "            try:\n",
    "                self.classifier_\n",
    "            except AttributeError:\n",
    "                warm_start = False\n",
    "        return warm_start\n",
    "\n",
    "    @staticmethod\n",
    "    def f1_metric_fn(labels, predictions):\n",
    "        p, p_op = tf.metrics.precision(labels=labels, predictions=predictions)\n",
    "        r, r_op = tf.metrics.recall(labels=labels, predictions=predictions)\n",
    "        return 2 * p * r / (p + r), tf.group(p_op, r_op)\n",
    "\n",
    "    def f1_score(self, labels, predictions):\n",
    "        return {\"f1-score\": self.f1_metric_fn(labels=labels, predictions=predictions)}\n",
    "\n",
    "    def model_fn(self, features, labels, mode, params):\n",
    "        # Network\n",
    "        logits = self.network_fn(features, params)\n",
    "\n",
    "        # Predict\n",
    "        predicted_classes = tf.argmax(logits, 1)\n",
    "        if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "            return tf.estimator.EstimatorSpec(mode, predictions=predicted_classes)\n",
    "\n",
    "        # Loss\n",
    "        class_M = self.label_encoder_.transform([\"M\"])\n",
    "        weights = tf.cast(tf.equal(labels, class_M), tf.float64)\n",
    "        weights = tf.multiply(weights, (self.weight_class_M - 1)) + 1\n",
    "        loss = tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=logits, weights=weights)\n",
    "\n",
    "        # Eval\n",
    "        if mode == tf.estimator.ModeKeys.EVAL:\n",
    "            return tf.estimator.EstimatorSpec(mode, loss=loss, predictions=predicted_classes)\n",
    "\n",
    "        # Train\n",
    "        optimizer = tf.train.AdagradOptimizer(learning_rate=self.learning_rate)\n",
    "        train_op = optimizer.minimize(loss, global_step=tf.train.get_global_step())\n",
    "        return tf.estimator.EstimatorSpec(mode, loss=loss, train_op=train_op)\n",
    "    \n",
    "    def pad_sentences(self, sentences):\n",
    "        return pad_sequences(sentences, self.sentence_length, padding=\"post\")\n",
    "        \n",
    "    @staticmethod\n",
    "    def sequences_lengths(X):\n",
    "        return X.shape[1] - np.argmax(X[:, ::-1] != 0, axis=1)\n",
    "\n",
    "    def input_fn(self, mode, X, y=None, num_epochs=1):\n",
    "        if mode in [tf.estimator.ModeKeys.TRAIN, tf.estimator.ModeKeys.EVAL]:\n",
    "            shuffle = True\n",
    "        else:\n",
    "            shuffle, num_epochs, y = (False, 1, None)\n",
    "            \n",
    "        lengths = self.sequences_lengths(X)\n",
    "        X = {self.features_key: X, self.seq_lengths_key: lengths}\n",
    "        \n",
    "        return tf.estimator.inputs.numpy_input_fn(X, y, batch_size=self.batch_size,\n",
    "                                                  num_epochs=num_epochs, shuffle=shuffle)\n",
    "\n",
    "    def create_dnn_classifier(self):\n",
    "        # Feature columns\n",
    "        self.feature_columns_ = [tf.feature_column.numeric_column(\n",
    "            key=self.features_key, shape=self.sentence_length)]\n",
    "        self.seq_lengths_column_ = [tf.feature_column.numeric_column(\n",
    "            key=self.seq_lengths_key)]\n",
    "\n",
    "        # Model\n",
    "        run_config = tf.estimator.RunConfig(model_dir=self.model_dir, log_step_count_steps=50)\n",
    "        model = tf.estimator.Estimator(model_fn=self.model_fn, config=run_config)\n",
    "        model = tf.contrib.estimator.add_metrics(model, self.f1_score)\n",
    "        return model\n",
    "\n",
    "    def apply_transformers(self, X, y):\n",
    "        X = self.pad_sentences(X)\n",
    "        y = self.label_encoder_.transform(y)\n",
    "        return X, y\n",
    "\n",
    "    def fit_and_apply_transformers(self, X, y):\n",
    "        X = self.pad_sentences(X)\n",
    "        self.label_encoder_ = LabelEncoder()\n",
    "        y = self.label_encoder_.fit_transform(y)\n",
    "        self.n_classes_ = len(self.label_encoder_.classes_)\n",
    "        return X, y\n",
    "\n",
    "    def fit(self, X, y, num_epochs=1, warm_start=True):\n",
    "        warm_start = self.check_warm_start(warm_start)\n",
    "        if not warm_start:\n",
    "            X, y = self.fit_and_apply_transformers(X, y)\n",
    "            self.classifier_ = self.create_dnn_classifier()\n",
    "        else:\n",
    "            X, y = self.apply_transformers(X, y)\n",
    "\n",
    "        self.classifier_.train(self.input_fn(tf.estimator.ModeKeys.TRAIN, X, y, num_epochs))\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        X = self.pad_sentences(X)\n",
    "        classes = list(self.classifier_.predict(self.input_fn(tf.estimator.ModeKeys.PREDICT, X)))\n",
    "        labels = self.label_encoder_.inverse_transform(classes)\n",
    "        return labels\n",
    "\n",
    "    def score(self, X, y):\n",
    "        X, y = self.apply_transformers(X, y)\n",
    "        results = self.classifier_.evaluate(self.input_fn(tf.estimator.ModeKeys.EVAL, X, y))\n",
    "        return results[\"f1-score\"]\n",
    "\n",
    "    def network_fn(self, features, params):        \n",
    "        # Create embedding matrix\n",
    "        embeddings = tf.convert_to_tensor(self.embeddings)\n",
    "        unknown_words_embedding = tf.Variable(tf.random_uniform(\n",
    "            [1, self.embedding_dim], -1.0, 1.0, tf.float64), trainable=True)\n",
    "        embeddings = tf.concat([embeddings, unknown_words_embedding], axis=0)\n",
    "\n",
    "        # Extract sequences embeddings\n",
    "        sequences = tf.feature_column.input_layer(features, self.feature_columns_)\n",
    "        embeddings = tf.nn.embedding_lookup(embeddings, tf.cast(sequences, tf.int64))\n",
    "\n",
    "        # Extract sequences lengths\n",
    "        cur_batch_size = tf.shape(embeddings)[0]\n",
    "        lengths = tf.feature_column.input_layer(features, self.seq_lengths_column_)\n",
    "        lengths = tf.cast(tf.reshape(lengths, [cur_batch_size]), tf.int32)\n",
    "\n",
    "        # LSTM layer with dropout on outputs\n",
    "        cell = tf.nn.rnn_cell.LSTMCell(self.num_units, activation=\"relu\")\n",
    "        cell = tf.nn.rnn_cell.DropoutWrapper(cell, output_keep_prob=self.dropout_keep_prob)\n",
    "        initial_state = cell.zero_state(cur_batch_size, tf.float64)\n",
    "        outputs, _ = tf.nn.dynamic_rnn(cell, embeddings, initial_state=initial_state, \n",
    "                                       sequence_length=lengths)\n",
    "\n",
    "        # Get last relevant output\n",
    "        index = tf.range(0, cur_batch_size) * self.sentence_length + (lengths - 1)\n",
    "        flat = tf.reshape(outputs, [-1, self.num_units])\n",
    "        relevant_output = tf.gather(flat, index)\n",
    "        \n",
    "        # Softmax\n",
    "        logits = tf.layers.dense(relevant_output, self.n_classes_, activation=None)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning rate and number of epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "\n",
    "n_splits, num_epochs = 3, 20\n",
    "learning_rates = [0.001, 0.005, 0.0075, 0.01, 0.025, 0.05, 0.075, 0.1]\n",
    "splitter = StratifiedKFold(n_splits, shuffle=True, random_state=1)\n",
    "results = {lr: {epoch: [] for epoch in range(num_epochs)} for lr in learning_rates}\n",
    "\n",
    "for lr in learning_rates:\n",
    "    for train_ind, test_ind in splitter.split(X_train, y_train):\n",
    "        model = LSTMModel(\n",
    "            weight_class_M=Counter(y_train)[\"C\"] / Counter(y_train)[\"M\"],\n",
    "            sentence_length=max(map(len, X_train)),\n",
    "            embeddings=embeddings,\n",
    "            num_units=50,\n",
    "            batch_size=128, \n",
    "            dropout_keep_prob=1.0,\n",
    "            learning_rate=lr,\n",
    "        )\n",
    "        for epoch in range(num_epochs):\n",
    "            with warnings.catch_warnings():\n",
    "                warnings.filterwarnings(\"ignore\")\n",
    "                model.fit(X_train[train_ind], y_train[train_ind])\n",
    "                score = model.score(X_train[test_ind], y_train[test_ind])\n",
    "                results[lr][epoch].append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# with open(\"lstm_results.p\", \"wb\") as f:\n",
    "#     pickle.dump(results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
