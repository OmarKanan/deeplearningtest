{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/omar/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.metrics import f1_score, make_scorer, confusion_matrix, \\\n",
    "    classification_report\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, \\\n",
    "    StratifiedShuffleSplit, RandomizedSearchCV, GridSearchCV, StratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"../Data/Learn/labels.pkl\", \"rb\") as f:\n",
    "    learn_labels = pickle.load(f)\n",
    "\n",
    "with open(\"../Data/generated/my_learn_sequences.pkl\", \"rb\") as f:\n",
    "    learn_sequences = pickle.load(f)\n",
    "\n",
    "with open(\"../Data/generated/my_embeddings.pkl\", \"rb\") as f:\n",
    "    embeddings = pickle.load(f)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    learn_sequences, learn_labels, test_size=0.3,\n",
    "    shuffle=True, stratify=learn_labels, random_state=42 + 2\n",
    ")\n",
    "X_train, X_test = np.array(X_train), np.array(X_test)\n",
    "y_train, y_test = np.array(y_train), np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28934, 300)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "class CNNModel(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self,\n",
    "                 sentence_length,\n",
    "                 embeddings,\n",
    "                 filters_by_ksize=50,\n",
    "                 kernel_sizes=(2,),\n",
    "                 batch_size=128,\n",
    "                 learning_rate=0.01,\n",
    "                 dropout_keep_prob=1.0,\n",
    "                 model_name=None,\n",
    "                 checkpoints_dir=\"../checkpoints/\",\n",
    "                 ):\n",
    "        self.sentence_length = sentence_length\n",
    "        self.embeddings = embeddings\n",
    "        self.embedding_dim = self.embeddings.shape[1]\n",
    "        self.filters_by_ksize = filters_by_ksize\n",
    "        self.kernel_sizes = kernel_sizes\n",
    "        self.batch_size = batch_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.dropout_keep_prob = dropout_keep_prob\n",
    "        self.features_key = \"x\"\n",
    "        self.weight_key = \"weight\"\n",
    "        self.model_dir = self.set_model_directory(checkpoints_dir, model_name)\n",
    "\n",
    "    @staticmethod\n",
    "    def set_model_directory(checkpoints_dir, model_name):\n",
    "        if model_name is not None:\n",
    "            model_dir = checkpoints_dir + model_name\n",
    "            # Check model_dir doesn't already exist\n",
    "            if os.path.exists(model_dir):\n",
    "                raise ValueError(\"model_dir already exists\")\n",
    "        else:\n",
    "            model_dir = None\n",
    "        return model_dir\n",
    "\n",
    "    def check_warm_start(self, warm_start):\n",
    "        if warm_start:\n",
    "            # Check if model was already fitted\n",
    "            try:\n",
    "                self.classifier_\n",
    "            except AttributeError:\n",
    "                warm_start = False\n",
    "        return warm_start\n",
    "\n",
    "    @staticmethod\n",
    "    def f1_metric_fn(labels, predictions):\n",
    "        p, p_op = tf.metrics.precision(labels=labels, predictions=predictions)\n",
    "        r, r_op = tf.metrics.recall(labels=labels, predictions=predictions)\n",
    "        return 2 * p * r / (p + r), tf.group(p_op, r_op)\n",
    "\n",
    "    def f1_score(self, labels, predictions):\n",
    "        return {\"f1-score\": self.f1_metric_fn(labels=labels, predictions=predictions)}\n",
    "\n",
    "    def network_fn(self, features, params):\n",
    "        # Create embedding matrix\n",
    "        embeddings = tf.convert_to_tensor(self.embeddings)\n",
    "        unknown_words_embedding = tf.Variable(tf.random_uniform(\n",
    "            [1, self.embedding_dim], -1.0, 1.0, tf.float64), trainable=True)\n",
    "        embeddings = tf.concat([embeddings, unknown_words_embedding], axis=0)\n",
    "\n",
    "        # Extract sequences embeddings\n",
    "        sequences = tf.feature_column.input_layer(features, params['feature_columns'])\n",
    "        embeddings = tf.nn.embedding_lookup(embeddings, tf.cast(sequences, tf.int64))\n",
    "\n",
    "        # Convolutions and max poolings\n",
    "        feature_maps = []\n",
    "        iterator = zip([self.filters_by_ksize] * len(self.kernel_sizes), self.kernel_sizes)\n",
    "        for filters, kernel_size in iterator:\n",
    "            tmp = tf.layers.conv1d(embeddings, filters, kernel_size, padding=\"same\")\n",
    "            tmp = tf.layers.max_pooling1d(tmp, [self.sentence_length], strides=1, padding=\"valid\")\n",
    "            feature_maps.append(tmp)\n",
    "\n",
    "        # Concat all feature maps, add dropout, and add softmax\n",
    "        shape = [-1, self.filters_by_ksize * len(self.kernel_sizes)]\n",
    "        feature_maps = tf.reshape(tf.concat(feature_maps, axis=2), shape)\n",
    "        feature_maps = tf.nn.dropout(feature_maps, self.dropout_keep_prob)\n",
    "        logits = tf.layers.dense(feature_maps, self.n_classes_, activation=None)\n",
    "        return logits\n",
    "\n",
    "    def model_fn(self, features, labels, mode, params):\n",
    "        # Network\n",
    "        logits = self.network_fn(features, params)\n",
    "\n",
    "        # Predict\n",
    "        predicted_classes = tf.argmax(logits, 1)\n",
    "        if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "            return tf.estimator.EstimatorSpec(mode, predictions=predicted_classes)\n",
    "\n",
    "        # Loss\n",
    "        class_M = self.label_encoder_.transform([\"M\"])\n",
    "        weights = tf.cast(tf.equal(labels, class_M), tf.float64)\n",
    "        weights = tf.multiply(weights, (6.63 - 1)) + 1\n",
    "        loss = tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=logits, weights=weights)\n",
    "\n",
    "        # Eval\n",
    "        if mode == tf.estimator.ModeKeys.EVAL:\n",
    "            return tf.estimator.EstimatorSpec(mode, loss=loss, predictions=predicted_classes)\n",
    "\n",
    "        # Train\n",
    "        optimizer = tf.train.AdagradOptimizer(learning_rate=self.learning_rate)\n",
    "        train_op = optimizer.minimize(loss, global_step=tf.train.get_global_step())\n",
    "        return tf.estimator.EstimatorSpec(mode, loss=loss, train_op=train_op)\n",
    "    \n",
    "    def input_fn(self, mode, X, y=None, num_epochs=1):\n",
    "        if mode in [tf.estimator.ModeKeys.TRAIN, tf.estimator.ModeKeys.EVAL]:\n",
    "            shuffle = True\n",
    "        else:\n",
    "            shuffle, num_epochs, y = (False, 1, None)\n",
    "        X = {self.features_key: X}\n",
    "        return tf.estimator.inputs.numpy_input_fn(X, y, self.batch_size, num_epochs, shuffle)\n",
    "\n",
    "    def create_dnn_classifier(self):\n",
    "        # Columns of X\n",
    "        self.feature_columns_ = [tf.feature_column.numeric_column(\n",
    "            key=self.features_key, shape=self.sentence_length)]\n",
    "\n",
    "        # Params\n",
    "        params = {\"feature_columns\": self.feature_columns_, \"n_classes\": self.n_classes_}\n",
    "        run_config = tf.estimator.RunConfig(model_dir=self.model_dir, log_step_count_steps=10)\n",
    "        \n",
    "        # Model\n",
    "        model = tf.estimator.Estimator(model_fn=self.model_fn, params=params, config=run_config)\n",
    "        model = tf.contrib.estimator.add_metrics(model, self.f1_score)\n",
    "        return model\n",
    "\n",
    "    def apply_transformers(self, X, y):\n",
    "        X = pad_sequences(X, self.sentence_length)\n",
    "        y = self.label_encoder_.transform(y)\n",
    "        return X, y\n",
    "\n",
    "    def fit_and_apply_transformers(self, X, y):\n",
    "        X = pad_sequences(X, self.sentence_length)\n",
    "        self.label_encoder_ = LabelEncoder()\n",
    "        y = self.label_encoder_.fit_transform(y)\n",
    "        self.n_classes_ = len(self.label_encoder_.classes_)\n",
    "        return X, y\n",
    "\n",
    "    def fit(self, X, y, num_epochs=1, warm_start=True):\n",
    "        warm_start = self.check_warm_start(warm_start)\n",
    "        if not warm_start:\n",
    "            X, y = self.fit_and_apply_transformers(X, y)\n",
    "            self.classifier_ = self.create_dnn_classifier()\n",
    "        else:\n",
    "            X, y = self.apply_transformers(X, y)\n",
    "        self.classifier_.train(self.input_fn(tf.estimator.ModeKeys.TRAIN, X, y, num_epochs))\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        X = pad_sequences(X, self.sentence_length)\n",
    "        classes = list(self.classifier_.predict(self.input_fn(tf.estimator.ModeKeys.PREDICT, X)))\n",
    "        labels = self.label_encoder_.inverse_transform(classes)\n",
    "        return labels\n",
    "\n",
    "    def score(self, X, y):\n",
    "        X, y = self.apply_transformers(X, y)\n",
    "        results = self.classifier_.evaluate(self.input_fn(tf.estimator.ModeKeys.EVAL, X, y))\n",
    "        return results[\"f1-score\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gs = GridSearchCV(\n",
    "    CNNModel(sentence_length=max(map(len, X_train)), embeddings=embeddings),\n",
    "    param_grid={\n",
    "        \"dropout_keep_prob\": [1.0, 0.9, 0.8, 0.7, 0.6, 0.5],\n",
    "        \"learning_rate\": [0.001, 0.005, 0.01, 0.05, 0.1],\n",
    "    }, \n",
    "    n_jobs=-1, \n",
    "    refit=False, \n",
    "    cv=3, \n",
    "    return_train_score=False\n",
    ")\n",
    "\n",
    "gs.fit(X_train, y_train, num_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>param_dropout_keep_prob</th>\n",
       "      <th>param_learning_rate</th>\n",
       "      <th>params</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>std_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2517.571017</td>\n",
       "      <td>40.272612</td>\n",
       "      <td>0.521074</td>\n",
       "      <td>1</td>\n",
       "      <td>0.05</td>\n",
       "      <td>{'dropout_keep_prob': 1.0, 'learning_rate': 0.05}</td>\n",
       "      <td>1</td>\n",
       "      <td>0.507187</td>\n",
       "      <td>0.543007</td>\n",
       "      <td>0.513028</td>\n",
       "      <td>1.255948</td>\n",
       "      <td>0.457329</td>\n",
       "      <td>0.015691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2521.973347</td>\n",
       "      <td>28.311614</td>\n",
       "      <td>0.514757</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.1</td>\n",
       "      <td>{'dropout_keep_prob': 0.9, 'learning_rate': 0.1}</td>\n",
       "      <td>2</td>\n",
       "      <td>0.516556</td>\n",
       "      <td>0.516984</td>\n",
       "      <td>0.510730</td>\n",
       "      <td>3.537341</td>\n",
       "      <td>2.068971</td>\n",
       "      <td>0.002853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2488.881290</td>\n",
       "      <td>44.128287</td>\n",
       "      <td>0.494524</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.05</td>\n",
       "      <td>{'dropout_keep_prob': 0.7, 'learning_rate': 0.05}</td>\n",
       "      <td>3</td>\n",
       "      <td>0.492471</td>\n",
       "      <td>0.492549</td>\n",
       "      <td>0.498552</td>\n",
       "      <td>4.388833</td>\n",
       "      <td>10.377894</td>\n",
       "      <td>0.002849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2507.087153</td>\n",
       "      <td>37.452126</td>\n",
       "      <td>0.492380</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.1</td>\n",
       "      <td>{'dropout_keep_prob': 0.8, 'learning_rate': 0.1}</td>\n",
       "      <td>4</td>\n",
       "      <td>0.489097</td>\n",
       "      <td>0.510522</td>\n",
       "      <td>0.477520</td>\n",
       "      <td>4.993876</td>\n",
       "      <td>2.774273</td>\n",
       "      <td>0.013671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2511.981449</td>\n",
       "      <td>38.861667</td>\n",
       "      <td>0.484987</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.05</td>\n",
       "      <td>{'dropout_keep_prob': 0.8, 'learning_rate': 0.05}</td>\n",
       "      <td>5</td>\n",
       "      <td>0.500743</td>\n",
       "      <td>0.522393</td>\n",
       "      <td>0.431818</td>\n",
       "      <td>17.379806</td>\n",
       "      <td>10.620683</td>\n",
       "      <td>0.038618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2517.895460</td>\n",
       "      <td>40.560565</td>\n",
       "      <td>0.478341</td>\n",
       "      <td>1</td>\n",
       "      <td>0.01</td>\n",
       "      <td>{'dropout_keep_prob': 1.0, 'learning_rate': 0.01}</td>\n",
       "      <td>6</td>\n",
       "      <td>0.479931</td>\n",
       "      <td>0.485823</td>\n",
       "      <td>0.469268</td>\n",
       "      <td>3.528913</td>\n",
       "      <td>1.850987</td>\n",
       "      <td>0.006851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2496.496011</td>\n",
       "      <td>36.983000</td>\n",
       "      <td>0.465365</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.05</td>\n",
       "      <td>{'dropout_keep_prob': 0.6, 'learning_rate': 0.05}</td>\n",
       "      <td>7</td>\n",
       "      <td>0.471088</td>\n",
       "      <td>0.472493</td>\n",
       "      <td>0.452512</td>\n",
       "      <td>12.869703</td>\n",
       "      <td>13.201122</td>\n",
       "      <td>0.009106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1249.867789</td>\n",
       "      <td>13.059176</td>\n",
       "      <td>0.452812</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.1</td>\n",
       "      <td>{'dropout_keep_prob': 0.5, 'learning_rate': 0.1}</td>\n",
       "      <td>8</td>\n",
       "      <td>0.481883</td>\n",
       "      <td>0.407701</td>\n",
       "      <td>0.468850</td>\n",
       "      <td>852.400119</td>\n",
       "      <td>3.262004</td>\n",
       "      <td>0.032339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2487.299340</td>\n",
       "      <td>35.334724</td>\n",
       "      <td>0.452049</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.01</td>\n",
       "      <td>{'dropout_keep_prob': 0.7, 'learning_rate': 0.01}</td>\n",
       "      <td>9</td>\n",
       "      <td>0.461778</td>\n",
       "      <td>0.450176</td>\n",
       "      <td>0.444191</td>\n",
       "      <td>0.798169</td>\n",
       "      <td>0.767798</td>\n",
       "      <td>0.007301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2525.847680</td>\n",
       "      <td>31.402206</td>\n",
       "      <td>0.448915</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.01</td>\n",
       "      <td>{'dropout_keep_prob': 0.9, 'learning_rate': 0.01}</td>\n",
       "      <td>10</td>\n",
       "      <td>0.446457</td>\n",
       "      <td>0.451007</td>\n",
       "      <td>0.449282</td>\n",
       "      <td>11.335145</td>\n",
       "      <td>9.321308</td>\n",
       "      <td>0.001876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2524.469412</td>\n",
       "      <td>31.573731</td>\n",
       "      <td>0.439517</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.01</td>\n",
       "      <td>{'dropout_keep_prob': 0.8, 'learning_rate': 0.01}</td>\n",
       "      <td>11</td>\n",
       "      <td>0.434451</td>\n",
       "      <td>0.446480</td>\n",
       "      <td>0.437621</td>\n",
       "      <td>2.700728</td>\n",
       "      <td>1.493539</td>\n",
       "      <td>0.005091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2494.333208</td>\n",
       "      <td>38.718111</td>\n",
       "      <td>0.432543</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.01</td>\n",
       "      <td>{'dropout_keep_prob': 0.6, 'learning_rate': 0.01}</td>\n",
       "      <td>12</td>\n",
       "      <td>0.417452</td>\n",
       "      <td>0.440455</td>\n",
       "      <td>0.439723</td>\n",
       "      <td>3.470115</td>\n",
       "      <td>3.235527</td>\n",
       "      <td>0.010675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2490.791174</td>\n",
       "      <td>48.307925</td>\n",
       "      <td>0.429984</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.1</td>\n",
       "      <td>{'dropout_keep_prob': 0.6, 'learning_rate': 0.1}</td>\n",
       "      <td>13</td>\n",
       "      <td>0.352292</td>\n",
       "      <td>0.460310</td>\n",
       "      <td>0.477360</td>\n",
       "      <td>3.379345</td>\n",
       "      <td>1.812448</td>\n",
       "      <td>0.055379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2519.955966</td>\n",
       "      <td>38.903115</td>\n",
       "      <td>0.429698</td>\n",
       "      <td>1</td>\n",
       "      <td>0.005</td>\n",
       "      <td>{'dropout_keep_prob': 1.0, 'learning_rate': 0....</td>\n",
       "      <td>14</td>\n",
       "      <td>0.414974</td>\n",
       "      <td>0.430737</td>\n",
       "      <td>0.443385</td>\n",
       "      <td>0.601931</td>\n",
       "      <td>0.128715</td>\n",
       "      <td>0.011622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2491.218569</td>\n",
       "      <td>40.268945</td>\n",
       "      <td>0.425721</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.1</td>\n",
       "      <td>{'dropout_keep_prob': 0.7, 'learning_rate': 0.1}</td>\n",
       "      <td>15</td>\n",
       "      <td>0.275207</td>\n",
       "      <td>0.513264</td>\n",
       "      <td>0.488711</td>\n",
       "      <td>5.190441</td>\n",
       "      <td>4.410189</td>\n",
       "      <td>0.106907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2492.191135</td>\n",
       "      <td>43.995659</td>\n",
       "      <td>0.423940</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.01</td>\n",
       "      <td>{'dropout_keep_prob': 0.5, 'learning_rate': 0.01}</td>\n",
       "      <td>16</td>\n",
       "      <td>0.414478</td>\n",
       "      <td>0.424192</td>\n",
       "      <td>0.433153</td>\n",
       "      <td>11.496090</td>\n",
       "      <td>7.637213</td>\n",
       "      <td>0.007626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2513.211870</td>\n",
       "      <td>25.349727</td>\n",
       "      <td>0.419579</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.05</td>\n",
       "      <td>{'dropout_keep_prob': 0.5, 'learning_rate': 0.05}</td>\n",
       "      <td>17</td>\n",
       "      <td>0.433361</td>\n",
       "      <td>0.372673</td>\n",
       "      <td>0.452706</td>\n",
       "      <td>3.647541</td>\n",
       "      <td>3.483078</td>\n",
       "      <td>0.034095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2519.886182</td>\n",
       "      <td>38.898974</td>\n",
       "      <td>0.418856</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>{'dropout_keep_prob': 1.0, 'learning_rate': 0.1}</td>\n",
       "      <td>18</td>\n",
       "      <td>0.295904</td>\n",
       "      <td>0.525977</td>\n",
       "      <td>0.434699</td>\n",
       "      <td>3.155077</td>\n",
       "      <td>1.466891</td>\n",
       "      <td>0.094595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2512.711489</td>\n",
       "      <td>43.753485</td>\n",
       "      <td>0.416115</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.005</td>\n",
       "      <td>{'dropout_keep_prob': 0.8, 'learning_rate': 0....</td>\n",
       "      <td>19</td>\n",
       "      <td>0.404278</td>\n",
       "      <td>0.427526</td>\n",
       "      <td>0.416542</td>\n",
       "      <td>7.443084</td>\n",
       "      <td>4.632988</td>\n",
       "      <td>0.009496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2515.194432</td>\n",
       "      <td>40.172882</td>\n",
       "      <td>0.415809</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.005</td>\n",
       "      <td>{'dropout_keep_prob': 0.9, 'learning_rate': 0....</td>\n",
       "      <td>20</td>\n",
       "      <td>0.410432</td>\n",
       "      <td>0.425476</td>\n",
       "      <td>0.411519</td>\n",
       "      <td>2.764094</td>\n",
       "      <td>1.010429</td>\n",
       "      <td>0.006850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2476.110465</td>\n",
       "      <td>44.467588</td>\n",
       "      <td>0.411354</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.005</td>\n",
       "      <td>{'dropout_keep_prob': 0.7, 'learning_rate': 0....</td>\n",
       "      <td>21</td>\n",
       "      <td>0.411607</td>\n",
       "      <td>0.408950</td>\n",
       "      <td>0.413506</td>\n",
       "      <td>2.817510</td>\n",
       "      <td>1.793002</td>\n",
       "      <td>0.001869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2496.556628</td>\n",
       "      <td>44.897061</td>\n",
       "      <td>0.402000</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.05</td>\n",
       "      <td>{'dropout_keep_prob': 0.9, 'learning_rate': 0.05}</td>\n",
       "      <td>22</td>\n",
       "      <td>0.451293</td>\n",
       "      <td>0.331643</td>\n",
       "      <td>0.423063</td>\n",
       "      <td>1.345436</td>\n",
       "      <td>1.525936</td>\n",
       "      <td>0.051068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2484.379335</td>\n",
       "      <td>49.239064</td>\n",
       "      <td>0.394522</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.005</td>\n",
       "      <td>{'dropout_keep_prob': 0.6, 'learning_rate': 0....</td>\n",
       "      <td>23</td>\n",
       "      <td>0.398883</td>\n",
       "      <td>0.390616</td>\n",
       "      <td>0.394067</td>\n",
       "      <td>3.295107</td>\n",
       "      <td>3.744913</td>\n",
       "      <td>0.003390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2496.275632</td>\n",
       "      <td>45.368809</td>\n",
       "      <td>0.387603</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.005</td>\n",
       "      <td>{'dropout_keep_prob': 0.5, 'learning_rate': 0....</td>\n",
       "      <td>24</td>\n",
       "      <td>0.378506</td>\n",
       "      <td>0.395307</td>\n",
       "      <td>0.388997</td>\n",
       "      <td>10.393972</td>\n",
       "      <td>11.644668</td>\n",
       "      <td>0.006930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2518.395723</td>\n",
       "      <td>39.718673</td>\n",
       "      <td>0.341355</td>\n",
       "      <td>1</td>\n",
       "      <td>0.001</td>\n",
       "      <td>{'dropout_keep_prob': 1.0, 'learning_rate': 0....</td>\n",
       "      <td>25</td>\n",
       "      <td>0.347262</td>\n",
       "      <td>0.327987</td>\n",
       "      <td>0.348816</td>\n",
       "      <td>0.999418</td>\n",
       "      <td>0.123201</td>\n",
       "      <td>0.009474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2519.104747</td>\n",
       "      <td>38.494648</td>\n",
       "      <td>0.317210</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.001</td>\n",
       "      <td>{'dropout_keep_prob': 0.9, 'learning_rate': 0....</td>\n",
       "      <td>26</td>\n",
       "      <td>0.302945</td>\n",
       "      <td>0.324418</td>\n",
       "      <td>0.324267</td>\n",
       "      <td>12.431682</td>\n",
       "      <td>7.243704</td>\n",
       "      <td>0.010087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2510.255144</td>\n",
       "      <td>38.726829</td>\n",
       "      <td>0.302490</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.001</td>\n",
       "      <td>{'dropout_keep_prob': 0.8, 'learning_rate': 0....</td>\n",
       "      <td>27</td>\n",
       "      <td>0.327348</td>\n",
       "      <td>0.285251</td>\n",
       "      <td>0.294867</td>\n",
       "      <td>7.437280</td>\n",
       "      <td>8.044645</td>\n",
       "      <td>0.018012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2502.600581</td>\n",
       "      <td>31.662423</td>\n",
       "      <td>0.290001</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.001</td>\n",
       "      <td>{'dropout_keep_prob': 0.6, 'learning_rate': 0....</td>\n",
       "      <td>28</td>\n",
       "      <td>0.300976</td>\n",
       "      <td>0.290563</td>\n",
       "      <td>0.278461</td>\n",
       "      <td>9.175234</td>\n",
       "      <td>5.451821</td>\n",
       "      <td>0.009200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2517.596235</td>\n",
       "      <td>27.072684</td>\n",
       "      <td>0.287103</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.001</td>\n",
       "      <td>{'dropout_keep_prob': 0.7, 'learning_rate': 0....</td>\n",
       "      <td>29</td>\n",
       "      <td>0.286682</td>\n",
       "      <td>0.269805</td>\n",
       "      <td>0.304823</td>\n",
       "      <td>7.575811</td>\n",
       "      <td>5.786406</td>\n",
       "      <td>0.014299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2509.911667</td>\n",
       "      <td>31.249348</td>\n",
       "      <td>0.281006</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.001</td>\n",
       "      <td>{'dropout_keep_prob': 0.5, 'learning_rate': 0....</td>\n",
       "      <td>30</td>\n",
       "      <td>0.268821</td>\n",
       "      <td>0.292882</td>\n",
       "      <td>0.281316</td>\n",
       "      <td>6.441368</td>\n",
       "      <td>3.866539</td>\n",
       "      <td>0.009826</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  mean_score_time  mean_test_score param_dropout_keep_prob  \\\n",
       "3     2517.571017        40.272612         0.521074                       1   \n",
       "9     2521.973347        28.311614         0.514757                     0.9   \n",
       "18    2488.881290        44.128287         0.494524                     0.7   \n",
       "14    2507.087153        37.452126         0.492380                     0.8   \n",
       "13    2511.981449        38.861667         0.484987                     0.8   \n",
       "2     2517.895460        40.560565         0.478341                       1   \n",
       "23    2496.496011        36.983000         0.465365                     0.6   \n",
       "29    1249.867789        13.059176         0.452812                     0.5   \n",
       "17    2487.299340        35.334724         0.452049                     0.7   \n",
       "7     2525.847680        31.402206         0.448915                     0.9   \n",
       "12    2524.469412        31.573731         0.439517                     0.8   \n",
       "22    2494.333208        38.718111         0.432543                     0.6   \n",
       "24    2490.791174        48.307925         0.429984                     0.6   \n",
       "1     2519.955966        38.903115         0.429698                       1   \n",
       "19    2491.218569        40.268945         0.425721                     0.7   \n",
       "27    2492.191135        43.995659         0.423940                     0.5   \n",
       "28    2513.211870        25.349727         0.419579                     0.5   \n",
       "4     2519.886182        38.898974         0.418856                       1   \n",
       "11    2512.711489        43.753485         0.416115                     0.8   \n",
       "6     2515.194432        40.172882         0.415809                     0.9   \n",
       "16    2476.110465        44.467588         0.411354                     0.7   \n",
       "8     2496.556628        44.897061         0.402000                     0.9   \n",
       "21    2484.379335        49.239064         0.394522                     0.6   \n",
       "26    2496.275632        45.368809         0.387603                     0.5   \n",
       "0     2518.395723        39.718673         0.341355                       1   \n",
       "5     2519.104747        38.494648         0.317210                     0.9   \n",
       "10    2510.255144        38.726829         0.302490                     0.8   \n",
       "20    2502.600581        31.662423         0.290001                     0.6   \n",
       "15    2517.596235        27.072684         0.287103                     0.7   \n",
       "25    2509.911667        31.249348         0.281006                     0.5   \n",
       "\n",
       "   param_learning_rate                                             params  \\\n",
       "3                 0.05  {'dropout_keep_prob': 1.0, 'learning_rate': 0.05}   \n",
       "9                  0.1   {'dropout_keep_prob': 0.9, 'learning_rate': 0.1}   \n",
       "18                0.05  {'dropout_keep_prob': 0.7, 'learning_rate': 0.05}   \n",
       "14                 0.1   {'dropout_keep_prob': 0.8, 'learning_rate': 0.1}   \n",
       "13                0.05  {'dropout_keep_prob': 0.8, 'learning_rate': 0.05}   \n",
       "2                 0.01  {'dropout_keep_prob': 1.0, 'learning_rate': 0.01}   \n",
       "23                0.05  {'dropout_keep_prob': 0.6, 'learning_rate': 0.05}   \n",
       "29                 0.1   {'dropout_keep_prob': 0.5, 'learning_rate': 0.1}   \n",
       "17                0.01  {'dropout_keep_prob': 0.7, 'learning_rate': 0.01}   \n",
       "7                 0.01  {'dropout_keep_prob': 0.9, 'learning_rate': 0.01}   \n",
       "12                0.01  {'dropout_keep_prob': 0.8, 'learning_rate': 0.01}   \n",
       "22                0.01  {'dropout_keep_prob': 0.6, 'learning_rate': 0.01}   \n",
       "24                 0.1   {'dropout_keep_prob': 0.6, 'learning_rate': 0.1}   \n",
       "1                0.005  {'dropout_keep_prob': 1.0, 'learning_rate': 0....   \n",
       "19                 0.1   {'dropout_keep_prob': 0.7, 'learning_rate': 0.1}   \n",
       "27                0.01  {'dropout_keep_prob': 0.5, 'learning_rate': 0.01}   \n",
       "28                0.05  {'dropout_keep_prob': 0.5, 'learning_rate': 0.05}   \n",
       "4                  0.1   {'dropout_keep_prob': 1.0, 'learning_rate': 0.1}   \n",
       "11               0.005  {'dropout_keep_prob': 0.8, 'learning_rate': 0....   \n",
       "6                0.005  {'dropout_keep_prob': 0.9, 'learning_rate': 0....   \n",
       "16               0.005  {'dropout_keep_prob': 0.7, 'learning_rate': 0....   \n",
       "8                 0.05  {'dropout_keep_prob': 0.9, 'learning_rate': 0.05}   \n",
       "21               0.005  {'dropout_keep_prob': 0.6, 'learning_rate': 0....   \n",
       "26               0.005  {'dropout_keep_prob': 0.5, 'learning_rate': 0....   \n",
       "0                0.001  {'dropout_keep_prob': 1.0, 'learning_rate': 0....   \n",
       "5                0.001  {'dropout_keep_prob': 0.9, 'learning_rate': 0....   \n",
       "10               0.001  {'dropout_keep_prob': 0.8, 'learning_rate': 0....   \n",
       "20               0.001  {'dropout_keep_prob': 0.6, 'learning_rate': 0....   \n",
       "15               0.001  {'dropout_keep_prob': 0.7, 'learning_rate': 0....   \n",
       "25               0.001  {'dropout_keep_prob': 0.5, 'learning_rate': 0....   \n",
       "\n",
       "    rank_test_score  split0_test_score  split1_test_score  split2_test_score  \\\n",
       "3                 1           0.507187           0.543007           0.513028   \n",
       "9                 2           0.516556           0.516984           0.510730   \n",
       "18                3           0.492471           0.492549           0.498552   \n",
       "14                4           0.489097           0.510522           0.477520   \n",
       "13                5           0.500743           0.522393           0.431818   \n",
       "2                 6           0.479931           0.485823           0.469268   \n",
       "23                7           0.471088           0.472493           0.452512   \n",
       "29                8           0.481883           0.407701           0.468850   \n",
       "17                9           0.461778           0.450176           0.444191   \n",
       "7                10           0.446457           0.451007           0.449282   \n",
       "12               11           0.434451           0.446480           0.437621   \n",
       "22               12           0.417452           0.440455           0.439723   \n",
       "24               13           0.352292           0.460310           0.477360   \n",
       "1                14           0.414974           0.430737           0.443385   \n",
       "19               15           0.275207           0.513264           0.488711   \n",
       "27               16           0.414478           0.424192           0.433153   \n",
       "28               17           0.433361           0.372673           0.452706   \n",
       "4                18           0.295904           0.525977           0.434699   \n",
       "11               19           0.404278           0.427526           0.416542   \n",
       "6                20           0.410432           0.425476           0.411519   \n",
       "16               21           0.411607           0.408950           0.413506   \n",
       "8                22           0.451293           0.331643           0.423063   \n",
       "21               23           0.398883           0.390616           0.394067   \n",
       "26               24           0.378506           0.395307           0.388997   \n",
       "0                25           0.347262           0.327987           0.348816   \n",
       "5                26           0.302945           0.324418           0.324267   \n",
       "10               27           0.327348           0.285251           0.294867   \n",
       "20               28           0.300976           0.290563           0.278461   \n",
       "15               29           0.286682           0.269805           0.304823   \n",
       "25               30           0.268821           0.292882           0.281316   \n",
       "\n",
       "    std_fit_time  std_score_time  std_test_score  \n",
       "3       1.255948        0.457329        0.015691  \n",
       "9       3.537341        2.068971        0.002853  \n",
       "18      4.388833       10.377894        0.002849  \n",
       "14      4.993876        2.774273        0.013671  \n",
       "13     17.379806       10.620683        0.038618  \n",
       "2       3.528913        1.850987        0.006851  \n",
       "23     12.869703       13.201122        0.009106  \n",
       "29    852.400119        3.262004        0.032339  \n",
       "17      0.798169        0.767798        0.007301  \n",
       "7      11.335145        9.321308        0.001876  \n",
       "12      2.700728        1.493539        0.005091  \n",
       "22      3.470115        3.235527        0.010675  \n",
       "24      3.379345        1.812448        0.055379  \n",
       "1       0.601931        0.128715        0.011622  \n",
       "19      5.190441        4.410189        0.106907  \n",
       "27     11.496090        7.637213        0.007626  \n",
       "28      3.647541        3.483078        0.034095  \n",
       "4       3.155077        1.466891        0.094595  \n",
       "11      7.443084        4.632988        0.009496  \n",
       "6       2.764094        1.010429        0.006850  \n",
       "16      2.817510        1.793002        0.001869  \n",
       "8       1.345436        1.525936        0.051068  \n",
       "21      3.295107        3.744913        0.003390  \n",
       "26     10.393972       11.644668        0.006930  \n",
       "0       0.999418        0.123201        0.009474  \n",
       "5      12.431682        7.243704        0.010087  \n",
       "10      7.437280        8.044645        0.018012  \n",
       "20      9.175234        5.451821        0.009200  \n",
       "15      7.575811        5.786406        0.014299  \n",
       "25      6.441368        3.866539        0.009826  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(gs.cv_results_).sort_values(\"rank_test_score\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "\n",
    "n_splits, num_epochs = 3, 20\n",
    "splitter = StratifiedKFold(n_splits, shuffle=True, random_state=1)\n",
    "results = {epoch: [] for epoch in range(num_epochs)}\n",
    "\n",
    "for cv, (train_ind, test_ind) in enumerate(splitter.split(X_train, y_train)):\n",
    "    print(\"Split %d...\" % cv)\n",
    "    model = CNNModel(\n",
    "        sentence_length=max(map(len, X_train)),\n",
    "        embeddings=embeddings,\n",
    "        filters_by_ksize=50, \n",
    "        kernel_sizes=(2,), \n",
    "        batch_size=128, \n",
    "        learning_rate=0.05,\n",
    "        dropout_keep_prob=1.0,\n",
    "    )\n",
    "    for epoch in range(num_epochs):\n",
    "        print(\"=> Epoch %d...\" % epoch)\n",
    "        model.fit(X_train[train_ind], y_train[train_ind])\n",
    "        score = model.score(X_train[test_ind], y_train[test_ind])\n",
    "        results[epoch].append(score)\n",
    "\n",
    "print(\"DONE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     0.331304\n",
       "1     0.451423\n",
       "2     0.378235\n",
       "3     0.392895\n",
       "4     0.503359\n",
       "5     0.455275\n",
       "6     0.472321\n",
       "7     0.404910\n",
       "8     0.426447\n",
       "9     0.466970\n",
       "10    0.488289\n",
       "11    0.498498\n",
       "12    0.452369\n",
       "13    0.490205\n",
       "14    0.511028\n",
       "15    0.509235\n",
       "16    0.512176\n",
       "17    0.505529\n",
       "18    0.467169\n",
       "19    0.496655\n",
       "dtype: float64"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(results).mean(axis=0) - pd.DataFrame(results).std(axis=0, ddof=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "\n",
    "n_splits, num_epochs = 3, 20\n",
    "splitter = StratifiedKFold(n_splits, shuffle=True, random_state=1)\n",
    "results2 = {epoch: [] for epoch in range(num_epochs)}\n",
    "\n",
    "for cv, (train_ind, test_ind) in enumerate(splitter.split(X_train, y_train)):\n",
    "    print(\"Split %d...\" % cv)\n",
    "    model = CNNModel(\n",
    "        sentence_length=max(map(len, X_train)),\n",
    "        embeddings=embeddings,\n",
    "        filters_by_ksize=50, \n",
    "        kernel_sizes=(2,), \n",
    "        batch_size=128, \n",
    "        learning_rate=0.1,\n",
    "        dropout_keep_prob=0.9,\n",
    "    )\n",
    "    for epoch in range(num_epochs):\n",
    "        print(\"=> Epoch %d...\" % epoch)\n",
    "        model.fit(X_train[train_ind], y_train[train_ind])\n",
    "        score = model.score(X_train[test_ind], y_train[test_ind])\n",
    "        results2[epoch].append(score)\n",
    "\n",
    "print(\"DONE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     0.285033\n",
       "1     0.361691\n",
       "2     0.293884\n",
       "3     0.413216\n",
       "4     0.500309\n",
       "5     0.425285\n",
       "6     0.392782\n",
       "7     0.366639\n",
       "8     0.423945\n",
       "9     0.467207\n",
       "10    0.503105\n",
       "11    0.428178\n",
       "12    0.453636\n",
       "13    0.499964\n",
       "14    0.490692\n",
       "15    0.468583\n",
       "16    0.465390\n",
       "17    0.400012\n",
       "18    0.477036\n",
       "19    0.402809\n",
       "dtype: float64"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(results2).mean(axis=0) - pd.DataFrame(results2).std(axis=0, ddof=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Choice:\n",
    "- learning_rate = 0.05\n",
    "- dropout = 1.0\n",
    "- num_epochs = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           C       0.94      0.90      0.92     11974\n",
      "           M       0.48      0.61      0.54      1805\n",
      "\n",
      "   micro avg       0.86      0.86      0.86     13779\n",
      "   macro avg       0.71      0.76      0.73     13779\n",
      "weighted avg       0.88      0.86      0.87     13779\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "\n",
    "model = CNNModel(\n",
    "    sentence_length=max(map(len, X_train)),\n",
    "    embeddings=embeddings,\n",
    "    filters_by_ksize=50, \n",
    "    kernel_sizes=(2,), \n",
    "    batch_size=128, \n",
    "    learning_rate=0.05,\n",
    "    dropout_keep_prob=1.0,\n",
    ")\n",
    "\n",
    "model.fit(X_train, y_train, num_epochs=5)\n",
    "print(classification_report(y_test, model.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paper configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split 0...\n",
      "=> Epoch 0... "
     ]
    }
   ],
   "source": [
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "\n",
    "n_splits, num_epochs = 3, 5\n",
    "splitter = StratifiedKFold(n_splits, shuffle=True, random_state=1)\n",
    "results3 = {epoch: [] for epoch in range(num_epochs)}\n",
    "\n",
    "for cv, (train_ind, test_ind) in enumerate(splitter.split(X_train, y_train)):\n",
    "    print(\"Split %d...\" % cv)\n",
    "    model = CNNModel(\n",
    "        sentence_length=max(map(len, X_train)),\n",
    "        embeddings=embeddings,\n",
    "        filters_by_ksize=100, \n",
    "        kernel_sizes=(3, 4, 5), \n",
    "        batch_size=50, \n",
    "        learning_rate=0.001,\n",
    "        dropout_keep_prob=0.5,\n",
    "    )\n",
    "    for epoch in range(num_epochs):\n",
    "        print(\"=> Epoch %d\" % epoch, end=\"... \")\n",
    "        model.fit(X_train[train_ind], y_train[train_ind])\n",
    "        score = model.score(X_train[test_ind], y_train[test_ind])\n",
    "        print(score)\n",
    "        results3[epoch].append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
