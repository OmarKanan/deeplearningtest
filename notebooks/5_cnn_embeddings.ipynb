{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/omar/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.metrics import f1_score, make_scorer, confusion_matrix, \\\n",
    "    classification_report\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, \\\n",
    "    StratifiedShuffleSplit, RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"../Data/Learn/labels.pkl\", \"rb\") as f:\n",
    "    learn_labels = pickle.load(f)\n",
    "\n",
    "with open(\"../Data/generated/my_learn_sequences.pkl\", \"rb\") as f:\n",
    "    learn_sequences = pickle.load(f)\n",
    "\n",
    "with open(\"../Data/generated/my_embeddings.pkl\", \"rb\") as f:\n",
    "    embeddings = pickle.load(f)\n",
    "\n",
    "# Remove unknown words row\n",
    "embeddings = embeddings[:-1, :]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    learn_sequences, learn_labels, test_size=0.3,\n",
    "    shuffle=True, stratify=learn_labels, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28935, 300)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DNNModel(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self,\n",
    "                 sentence_length,\n",
    "                 embeddings,\n",
    "                 filters_by_ksize=5,\n",
    "                 kernel_sizes=(2,),\n",
    "                 batch_size=128,\n",
    "                 learning_rate=0.1,\n",
    "                 dropout_keep_prob=1.0,\n",
    "                 model_name=None,\n",
    "                 checkpoints_dir=\"../checkpoints/\",\n",
    "                 ):\n",
    "        self.sentence_length = sentence_length\n",
    "        self.embeddings = embeddings\n",
    "        self.embedding_dim = self.embeddings.shape[1]\n",
    "        self.filters_by_ksize = filters_by_ksize\n",
    "        self.kernel_sizes = kernel_sizes\n",
    "        self.batch_size = batch_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.dropout_keep_prob = dropout_keep_prob\n",
    "        self.features_key = \"x\"\n",
    "        self.weight_key = \"weight\"\n",
    "        self.set_model_directory(checkpoints_dir, model_name)\n",
    "\n",
    "    def set_model_directory(self, checkpoints_dir, model_name):\n",
    "        if model_name is not None:\n",
    "            self.model_dir = checkpoints_dir + model_name\n",
    "            # Check model_dir doesn't already exist\n",
    "            if os.path.exists(self.model_dir):\n",
    "                raise ValueError(\"model_dir already exists\")\n",
    "        else:\n",
    "            self.model_dir = None\n",
    "\n",
    "    def check_warm_start(self, warm_start):\n",
    "        if warm_start:\n",
    "            # Check if model was already fitted\n",
    "            try:\n",
    "                self.classifier_\n",
    "            except:\n",
    "                warm_start = False\n",
    "        return warm_start\n",
    "    \n",
    "    def create_dnn_classifier(self):\n",
    "        # Columns of X\n",
    "        self.feature_columns_ = [tf.feature_column.numeric_column(\n",
    "            key=self.features_key, shape=self.sentence_length\n",
    "        )]\n",
    "        # Model parameters\n",
    "        params = {\n",
    "            \"feature_columns\": self.feature_columns_,\n",
    "            \"n_classes\": self.n_classes_,\n",
    "        }\n",
    "        # Config\n",
    "        run_config = tf.estimator.RunConfig(\n",
    "            model_dir=self.model_dir,\n",
    "            log_step_count_steps=10,\n",
    "        )\n",
    "        # Create model\n",
    "        model = tf.estimator.Estimator(model_fn=self.model_fn,\n",
    "                                       model_dir=self.model_dir,\n",
    "                                       params=params,\n",
    "                                       config=run_config)\n",
    "        model = tf.contrib.estimator.add_metrics(model, self.f1_score)\n",
    "        return model\n",
    "        \n",
    "    def f1_score(self, labels, predictions):\n",
    "        return {\"f1-score\": self.f1_metric_fn(labels=labels, predictions=predictions)}\n",
    "    \n",
    "    def f1_metric_fn(self, labels, predictions):\n",
    "        p, p_op = tf.metrics.precision(labels=labels, predictions=predictions)\n",
    "        r, r_op = tf.metrics.recall(labels=labels, predictions=predictions)\n",
    "        return 2 * p * r / (p + r), tf.group(p_op, r_op)        \n",
    "\n",
    "    def input_fn(self, mode, X, y=None, num_epochs=1):\n",
    "        if mode in [tf.estimator.ModeKeys.TRAIN, tf.estimator.ModeKeys.EVAL]:\n",
    "            shuffle = True\n",
    "        else:\n",
    "            shuffle, num_epochs, y = (False, 1, None)\n",
    "        X = {self.features_key: X}\n",
    "        return tf.estimator.inputs.numpy_input_fn(X, y, self.batch_size,\n",
    "                                                  num_epochs, shuffle)\n",
    "\n",
    "    def model_fn(self, features, labels, mode, params):\n",
    "        # Network\n",
    "        logits = self.network_fn(features, params)\n",
    "        \n",
    "        # Predict\n",
    "        predicted_classes = tf.argmax(logits, 1)\n",
    "        if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "            return tf.estimator.EstimatorSpec(mode, predictions=predicted_classes)        \n",
    "        \n",
    "        # Loss\n",
    "        class_M = self.label_encoder_.transform([\"M\"])\n",
    "        weights = tf.cast(tf.equal(labels, class_M), tf.float64)\n",
    "        weights = tf.multiply(weights, (6.63 - 1)) + 1\n",
    "        loss = tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=logits,\n",
    "                                                      weights=weights)\n",
    "        \n",
    "        # Eval\n",
    "        if mode == tf.estimator.ModeKeys.EVAL:\n",
    "            return tf.estimator.EstimatorSpec(mode, loss=loss, predictions=predicted_classes)\n",
    "        \n",
    "        # Train\n",
    "        optimizer = tf.train.AdagradOptimizer(learning_rate=self.learning_rate)\n",
    "        train_op = optimizer.minimize(loss, global_step=tf.train.get_global_step())\n",
    "        return tf.estimator.EstimatorSpec(mode, loss=loss, train_op=train_op)\n",
    "    \n",
    "    def network_fn(self, features, params):\n",
    "        # Create embedding matrix\n",
    "        embeddings = tf.convert_to_tensor(self.embeddings)\n",
    "        unknown_words_embedding = tf.Variable(tf.random_uniform(\n",
    "            [1, self.embedding_dim], -1.0, 1.0, tf.float64), trainable=True)\n",
    "        embeddings = tf.concat([embeddings, unknown_words_embedding], axis=0)\n",
    "        \n",
    "        # Extract sequences embeddings\n",
    "        sequences = tf.feature_column.input_layer(features, params['feature_columns'])\n",
    "        embeddings = tf.nn.embedding_lookup(embeddings, tf.cast(sequences, tf.int64))\n",
    "        \n",
    "        # Convolutions and max poolings\n",
    "        feature_maps = []\n",
    "        iterator = zip([self.filters_by_ksize] * len(self.kernel_sizes), self.kernel_sizes)\n",
    "        for filters, kernel_size in iterator:\n",
    "            tmp = tf.layers.conv1d(embeddings, filters, kernel_size, padding=\"same\")\n",
    "            tmp = tf.layers.max_pooling1d(tmp, [self.sentence_length], strides=1, \n",
    "                                          padding=\"valid\")\n",
    "            feature_maps.append(tmp)\n",
    "        \n",
    "        # Concat all feature maps and add softmax\n",
    "        shape = [-1, self.filters_by_ksize * len(self.kernel_sizes)]\n",
    "        feature_maps = tf.reshape(tf.concat(feature_maps, axis=2), shape)\n",
    "        feature_maps = tf.nn.dropout(feature_maps, self.dropout_keep_prob)\n",
    "        logits = tf.layers.dense(feature_maps, self.n_classes_, activation=None)\n",
    "        return logits\n",
    "    \n",
    "    def fit_and_apply_transformers(self, X, y):\n",
    "        X = pad_sequences(X, self.sentence_length)\n",
    "        self.label_encoder_ = LabelEncoder()\n",
    "        y = self.label_encoder_.fit_transform(y)\n",
    "        self.n_classes_ = len(self.label_encoder_.classes_)\n",
    "        return X, y\n",
    "\n",
    "    def apply_transformers(self, X, y):\n",
    "        X = pad_sequences(X, self.sentence_length)\n",
    "        y = self.label_encoder_.transform(y)\n",
    "        return X, y\n",
    "\n",
    "    def fit(self, X, y, num_epochs=1, warm_start=True):\n",
    "        warm_start = self.check_warm_start(warm_start)\n",
    "        if not warm_start:\n",
    "            X, y = self.fit_and_apply_transformers(X, y)\n",
    "            self.classifier_ = self.create_dnn_classifier()\n",
    "        else:\n",
    "            X, y = self.apply_transformers(X, y)\n",
    "\n",
    "        self.classifier_.train(self.input_fn(\n",
    "            tf.estimator.ModeKeys.TRAIN, X, y, num_epochs))\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        X = pad_sequences(X, self.sentence_length)\n",
    "        classes = list(self.classifier_.predict(self.input_fn(\n",
    "            tf.estimator.ModeKeys.PREDICT, X)))\n",
    "        labels = self.label_encoder_.inverse_transform(classes)\n",
    "        return labels\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        X, y = self.apply_transformers(X, y)\n",
    "        results = self.classifier_.evaluate(self.input_fn(\n",
    "            tf.estimator.ModeKeys.EVAL, X, y))\n",
    "        return results[\"f1-score\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### kernel_sizes=(2,), filters_by_ksize=50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%capture capt\n",
    "\n",
    "for lr in np.logspace(-6, 0, 7):\n",
    "    print(\"-\" * 80 + \"\\nLearning rate :\", lr)\n",
    "    model = DNNModel(\n",
    "        sentence_length=max(map(len, X_train)), \n",
    "        embeddings=embeddings,\n",
    "        dropout_keep_prob=1.0,\n",
    "        filters_by_ksize=50,\n",
    "        kernel_sizes=(2,),\n",
    "        learning_rate=lr,\n",
    "    )\n",
    "    for epoch in range(5):\n",
    "        model.fit(X_train, y_train, num_epochs=1, warm_start=True)\n",
    "        f1 = model.score(X_test, y_test)\n",
    "        print(\"EPOCH %d: test f1-score = %.3f\" % (epoch, f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Learning rate : 1e-06\n",
      "EPOCH 0: test f1-score = nan\n",
      "EPOCH 1: test f1-score = nan\n",
      "EPOCH 2: test f1-score = nan\n",
      "EPOCH 3: test f1-score = nan\n",
      "EPOCH 4: test f1-score = nan\n",
      "--------------------------------------------------------------------------------\n",
      "Learning rate : 1e-05\n",
      "EPOCH 0: test f1-score = nan\n",
      "EPOCH 1: test f1-score = nan\n",
      "EPOCH 2: test f1-score = nan\n",
      "EPOCH 3: test f1-score = nan\n",
      "EPOCH 4: test f1-score = nan\n",
      "--------------------------------------------------------------------------------\n",
      "Learning rate : 0.0001\n",
      "EPOCH 0: test f1-score = 0.225\n",
      "EPOCH 1: test f1-score = 0.234\n",
      "EPOCH 2: test f1-score = 0.235\n",
      "EPOCH 3: test f1-score = 0.240\n",
      "EPOCH 4: test f1-score = 0.243\n",
      "--------------------------------------------------------------------------------\n",
      "Learning rate : 0.001\n",
      "EPOCH 0: test f1-score = 0.270\n",
      "EPOCH 1: test f1-score = 0.285\n",
      "EPOCH 2: test f1-score = 0.306\n",
      "EPOCH 3: test f1-score = 0.321\n",
      "EPOCH 4: test f1-score = 0.323\n",
      "--------------------------------------------------------------------------------\n",
      "Learning rate : 0.01\n",
      "EPOCH 0: test f1-score = 0.345\n",
      "EPOCH 1: test f1-score = 0.300\n",
      "EPOCH 2: test f1-score = 0.456\n",
      "EPOCH 3: test f1-score = 0.457\n",
      "EPOCH 4: test f1-score = 0.453\n",
      "--------------------------------------------------------------------------------\n",
      "Learning rate : 0.1\n",
      "EPOCH 0: test f1-score = 0.332\n",
      "EPOCH 1: test f1-score = 0.329\n",
      "EPOCH 2: test f1-score = 0.519\n",
      "EPOCH 3: test f1-score = 0.479\n",
      "EPOCH 4: test f1-score = 0.393\n",
      "--------------------------------------------------------------------------------\n",
      "Learning rate : 1.0\n",
      "EPOCH 0: test f1-score = 0.336\n",
      "EPOCH 1: test f1-score = 0.408\n",
      "EPOCH 2: test f1-score = 0.432\n",
      "EPOCH 3: test f1-score = 0.369\n",
      "EPOCH 4: test f1-score = 0.423\n"
     ]
    }
   ],
   "source": [
    "capt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most stable: learning rate = 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../Data/Learn/labels.pkl\", \"rb\") as f:\n",
    "    learn_labels = pickle.load(f)\n",
    "\n",
    "with open(\"../Data/generated/my_learn_sequences.pkl\", \"rb\") as f:\n",
    "    learn_sequences = pickle.load(f)\n",
    "\n",
    "with open(\"../Data/generated/my_embeddings.pkl\", \"rb\") as f:\n",
    "    embeddings = pickle.load(f)\n",
    "\n",
    "# Remove unknown words row\n",
    "embeddings = embeddings[:-1, :]\n",
    "\n",
    "# Different random state\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    learn_sequences, learn_labels, test_size=0.3,\n",
    "    shuffle=True, stratify=learn_labels, random_state=42 + 1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/omar/Notebooks/deeplearningtest\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cnn_model import CNNModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CNNModel(batch_size=128, checkpoints_dir=None, dropout_keep_prob=1.0,\n",
       "     embeddings=array([[ 0.01856,  0.06263, ..., -0.13433,  0.14924],\n",
       "       [-0.07446, -0.01878, ..., -0.08351, -0.09113],\n",
       "       ...,\n",
       "       [-0.74394, -0.07888, ...,  0.59589, -0.16601],\n",
       "       [ 0.01831,  0.3224 , ..., -0.15861, -0.00104]]),\n",
       "     filters_by_ksize=50, kernel_sizes=(2,), learning_rate=0.01,\n",
       "     model_name=None, sentence_length=379)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = CNNModel(sentence_length=max(map(len, X_train)), embeddings=embeddings)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using temporary folder as model directory: /home/omar/tmp/tmphtt2bsg4\n",
      "INFO:tensorflow:Using config: {'_model_dir': '/home/omar/tmp/tmphtt2bsg4', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 10, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7ff7603fd898>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "INFO:tensorflow:Using config: {'_model_dir': '/home/omar/tmp/tmphtt2bsg4', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 10, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7ff7603fd400>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 0 into /home/omar/tmp/tmphtt2bsg4/model.ckpt.\n",
      "INFO:tensorflow:loss = 1.3670070171356201, step = 1\n",
      "INFO:tensorflow:global_step/sec: 4.32889\n",
      "INFO:tensorflow:loss = 1.1078540086746216, step = 11 (2.311 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.99352\n",
      "INFO:tensorflow:loss = 1.2307170629501343, step = 21 (1.668 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.99386\n",
      "INFO:tensorflow:loss = 1.1221377849578857, step = 31 (1.668 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.99653\n",
      "INFO:tensorflow:loss = 1.16410493850708, step = 41 (1.668 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.01598\n",
      "INFO:tensorflow:loss = 1.045832872390747, step = 51 (1.662 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.99041\n",
      "INFO:tensorflow:loss = 1.2392973899841309, step = 61 (1.669 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.93496\n",
      "INFO:tensorflow:loss = 1.2571808099746704, step = 71 (1.685 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.94565\n",
      "INFO:tensorflow:loss = 1.1788067817687988, step = 81 (1.682 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.82944\n",
      "INFO:tensorflow:loss = 0.944952666759491, step = 91 (1.716 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.92648\n",
      "INFO:tensorflow:loss = 1.0861201286315918, step = 101 (1.687 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.94166\n",
      "INFO:tensorflow:loss = 1.1791632175445557, step = 111 (1.683 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.9069\n",
      "INFO:tensorflow:loss = 1.0719144344329834, step = 121 (1.693 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.94496\n",
      "INFO:tensorflow:loss = 1.138685703277588, step = 131 (1.682 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.91474\n",
      "INFO:tensorflow:loss = 1.289421558380127, step = 141 (1.691 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.90575\n",
      "INFO:tensorflow:loss = 1.1173896789550781, step = 151 (1.693 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.94564\n",
      "INFO:tensorflow:loss = 1.0251723527908325, step = 161 (1.682 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.96995\n",
      "INFO:tensorflow:loss = 1.097078800201416, step = 171 (1.675 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.92431\n",
      "INFO:tensorflow:loss = 0.9046580791473389, step = 181 (1.688 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.95804\n",
      "INFO:tensorflow:loss = 1.1215109825134277, step = 191 (1.678 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.95954\n",
      "INFO:tensorflow:loss = 0.9080410003662109, step = 201 (1.678 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.91902\n",
      "INFO:tensorflow:loss = 1.2602717876434326, step = 211 (1.689 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.92921\n",
      "INFO:tensorflow:loss = 1.0105372667312622, step = 221 (1.687 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.93432\n",
      "INFO:tensorflow:loss = 1.0856058597564697, step = 231 (1.685 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.93152\n",
      "INFO:tensorflow:loss = 0.9707283973693848, step = 241 (1.686 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.97357\n",
      "INFO:tensorflow:loss = 0.9477613568305969, step = 251 (1.674 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.94876\n",
      "INFO:tensorflow:loss = 1.017699122428894, step = 261 (1.681 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.91042\n",
      "INFO:tensorflow:loss = 1.0792334079742432, step = 271 (1.692 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.94463\n",
      "INFO:tensorflow:loss = 1.2661182880401611, step = 281 (1.682 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.9816\n",
      "INFO:tensorflow:loss = 1.1326754093170166, step = 291 (1.672 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.94579\n",
      "INFO:tensorflow:loss = 0.995987057685852, step = 301 (1.682 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.93735\n",
      "INFO:tensorflow:loss = 0.8960740566253662, step = 311 (1.684 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.89436\n",
      "INFO:tensorflow:loss = 1.0045866966247559, step = 321 (1.697 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.94121\n",
      "INFO:tensorflow:loss = 0.9586172103881836, step = 331 (1.683 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.94569\n",
      "INFO:tensorflow:loss = 1.0964691638946533, step = 341 (1.682 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.91333\n",
      "INFO:tensorflow:loss = 1.094613790512085, step = 351 (1.691 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.92424\n",
      "INFO:tensorflow:loss = 1.127778172492981, step = 361 (1.688 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.92787\n",
      "INFO:tensorflow:loss = 1.0299330949783325, step = 371 (1.687 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.92223\n",
      "INFO:tensorflow:loss = 0.8322511315345764, step = 381 (1.689 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.94842\n",
      "INFO:tensorflow:loss = 0.9139524698257446, step = 391 (1.681 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.88966\n",
      "INFO:tensorflow:loss = 0.9298872351646423, step = 401 (1.698 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.95467\n",
      "INFO:tensorflow:loss = 0.9065151214599609, step = 411 (1.679 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.9083\n",
      "INFO:tensorflow:loss = 1.0994389057159424, step = 421 (1.693 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.95574\n",
      "INFO:tensorflow:loss = 1.0662649869918823, step = 431 (1.679 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.95238\n",
      "INFO:tensorflow:loss = 0.8960562348365784, step = 441 (1.680 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.88294\n",
      "INFO:tensorflow:loss = 0.8957149386405945, step = 451 (1.700 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.94757\n",
      "INFO:tensorflow:loss = 0.8196017146110535, step = 461 (1.681 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.92136\n",
      "INFO:tensorflow:loss = 0.9859169721603394, step = 471 (1.689 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.95422\n",
      "INFO:tensorflow:loss = 1.1862000226974487, step = 481 (1.679 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.95429\n",
      "INFO:tensorflow:loss = 0.9490623474121094, step = 491 (1.679 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.94446\n",
      "INFO:tensorflow:loss = 0.8823692798614502, step = 501 (1.683 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.91366\n",
      "INFO:tensorflow:loss = 1.2468658685684204, step = 511 (1.691 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.93747\n",
      "INFO:tensorflow:loss = 1.1574815511703491, step = 521 (1.684 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.94643\n",
      "INFO:tensorflow:loss = 1.008846402168274, step = 531 (1.682 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.94073\n",
      "INFO:tensorflow:loss = 0.9533987045288086, step = 541 (1.683 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.97175\n",
      "INFO:tensorflow:loss = 0.964202344417572, step = 551 (1.675 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.89637\n",
      "INFO:tensorflow:loss = 0.9761480093002319, step = 561 (1.696 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.92307\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 0.9160079956054688, step = 571 (1.688 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.94085\n",
      "INFO:tensorflow:loss = 1.03570556640625, step = 581 (1.683 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.95396\n",
      "INFO:tensorflow:loss = 0.9967358112335205, step = 591 (1.680 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.96804\n",
      "INFO:tensorflow:loss = 1.0164092779159546, step = 601 (1.676 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.95963\n",
      "INFO:tensorflow:loss = 0.8200972080230713, step = 611 (1.678 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.93108\n",
      "INFO:tensorflow:loss = 0.9402025938034058, step = 621 (1.686 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.95099\n",
      "INFO:tensorflow:loss = 0.9440509676933289, step = 631 (1.680 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.98116\n",
      "INFO:tensorflow:loss = 1.0836787223815918, step = 641 (1.672 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.92209\n",
      "INFO:tensorflow:loss = 0.7479866147041321, step = 651 (1.689 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.90614\n",
      "INFO:tensorflow:loss = 0.6996512413024902, step = 661 (1.693 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.94469\n",
      "INFO:tensorflow:loss = 0.8441296815872192, step = 671 (1.682 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.91385\n",
      "INFO:tensorflow:loss = 0.8202544450759888, step = 681 (1.691 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.96808\n",
      "INFO:tensorflow:loss = 1.0782020092010498, step = 691 (1.676 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.9382\n",
      "INFO:tensorflow:loss = 1.027489185333252, step = 701 (1.684 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.91803\n",
      "INFO:tensorflow:loss = 0.8505094051361084, step = 711 (1.690 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.92926\n",
      "INFO:tensorflow:loss = 1.126686930656433, step = 721 (1.686 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.95265\n",
      "INFO:tensorflow:loss = 1.0126904249191284, step = 731 (1.680 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.95923\n",
      "INFO:tensorflow:loss = 1.0387619733810425, step = 741 (1.678 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.93007\n",
      "INFO:tensorflow:loss = 0.782390832901001, step = 751 (1.686 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.89482\n",
      "INFO:tensorflow:loss = 0.8611623048782349, step = 761 (1.696 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.94573\n",
      "INFO:tensorflow:loss = 1.0304981470108032, step = 771 (1.682 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.9761\n",
      "INFO:tensorflow:loss = 0.7961850166320801, step = 781 (1.673 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.94261\n",
      "INFO:tensorflow:loss = 0.8140969276428223, step = 791 (1.683 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.92772\n",
      "INFO:tensorflow:loss = 0.7540845274925232, step = 801 (1.687 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.87647\n",
      "INFO:tensorflow:loss = 0.8797249794006348, step = 811 (1.702 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.97292\n",
      "INFO:tensorflow:loss = 0.7356364130973816, step = 821 (1.674 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.94306\n",
      "INFO:tensorflow:loss = 1.008030891418457, step = 831 (1.683 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.96778\n",
      "INFO:tensorflow:loss = 0.8767129182815552, step = 841 (1.676 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.96055\n",
      "INFO:tensorflow:loss = 1.171473503112793, step = 851 (1.678 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.90587\n",
      "INFO:tensorflow:loss = 1.017049789428711, step = 861 (1.693 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.94694\n",
      "INFO:tensorflow:loss = 0.8548349738121033, step = 871 (1.682 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.98141\n",
      "INFO:tensorflow:loss = 0.820704460144043, step = 881 (1.672 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.00188\n",
      "INFO:tensorflow:loss = 1.1430909633636475, step = 891 (1.666 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.98178\n",
      "INFO:tensorflow:loss = 0.9807273149490356, step = 901 (1.672 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.95431\n",
      "INFO:tensorflow:loss = 1.0125553607940674, step = 911 (1.679 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.88168\n",
      "INFO:tensorflow:loss = 0.9256086349487305, step = 921 (1.700 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.96545\n",
      "INFO:tensorflow:loss = 0.7092912197113037, step = 931 (1.676 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.9374\n",
      "INFO:tensorflow:loss = 0.9276507496833801, step = 941 (1.684 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.94777\n",
      "INFO:tensorflow:loss = 0.6661871075630188, step = 951 (1.681 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.93395\n",
      "INFO:tensorflow:loss = 0.9903520941734314, step = 961 (1.685 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.94294\n",
      "INFO:tensorflow:loss = 0.9299137592315674, step = 971 (1.683 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.95371\n",
      "INFO:tensorflow:loss = 0.8972102403640747, step = 981 (1.680 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.96437\n",
      "INFO:tensorflow:loss = 0.7716227173805237, step = 991 (1.677 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.96706\n",
      "INFO:tensorflow:loss = 0.8713885545730591, step = 1001 (1.676 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.9691\n",
      "INFO:tensorflow:loss = 0.8742802143096924, step = 1011 (1.675 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.96637\n",
      "INFO:tensorflow:loss = 0.6514360904693604, step = 1021 (1.676 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.87843\n",
      "INFO:tensorflow:loss = 0.9473334550857544, step = 1031 (1.701 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.98355\n",
      "INFO:tensorflow:loss = 0.9273136854171753, step = 1041 (1.671 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.90993\n",
      "INFO:tensorflow:loss = 0.8448212742805481, step = 1051 (1.692 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.95103\n",
      "INFO:tensorflow:loss = 0.8117851614952087, step = 1061 (1.680 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.98721\n",
      "INFO:tensorflow:loss = 0.9434421062469482, step = 1071 (1.670 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.00234\n",
      "INFO:tensorflow:loss = 0.8392937183380127, step = 1081 (1.666 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.97406\n",
      "INFO:tensorflow:loss = 0.7658504843711853, step = 1091 (1.674 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.92092\n",
      "INFO:tensorflow:loss = 0.856831431388855, step = 1101 (1.689 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.93585\n",
      "INFO:tensorflow:loss = 0.9492249488830566, step = 1111 (1.684 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.95436\n",
      "INFO:tensorflow:loss = 0.6929693818092346, step = 1121 (1.680 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.92663\n",
      "INFO:tensorflow:loss = 0.8530598282814026, step = 1131 (1.687 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.9841\n",
      "INFO:tensorflow:loss = 1.0429961681365967, step = 1141 (1.671 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.92251\n",
      "INFO:tensorflow:loss = 0.9485183358192444, step = 1151 (1.689 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.85731\n",
      "INFO:tensorflow:loss = 0.809520423412323, step = 1161 (1.707 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.02746\n",
      "INFO:tensorflow:loss = 0.7560875415802002, step = 1171 (1.659 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.01018\n",
      "INFO:tensorflow:loss = 0.9077739715576172, step = 1181 (1.664 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.91665\n",
      "INFO:tensorflow:loss = 0.7015171051025391, step = 1191 (1.690 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.92796\n",
      "INFO:tensorflow:loss = 0.9013225436210632, step = 1201 (1.687 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.95312\n",
      "INFO:tensorflow:loss = 1.1075143814086914, step = 1211 (1.680 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.94707\n",
      "INFO:tensorflow:loss = 1.31050443649292, step = 1221 (1.682 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.94824\n",
      "INFO:tensorflow:loss = 0.9002928733825684, step = 1231 (1.681 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.939\n",
      "INFO:tensorflow:loss = 0.6971414685249329, step = 1241 (1.684 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.93649\n",
      "INFO:tensorflow:loss = 0.7826180458068848, step = 1251 (1.684 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1256 into /home/omar/tmp/tmphtt2bsg4/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.8536859750747681.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2018-11-18-17:37:18\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /home/omar/tmp/tmphtt2bsg4/model.ckpt-1256\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2018-11-18-17:37:25\n",
      "INFO:tensorflow:Saving dict for global step 1256: f1-score = 0.4656185, global_step = 1256, loss = 0.89818144\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 1256: /home/omar/tmp/tmphtt2bsg4/model.ckpt-1256\n",
      "INFO:tensorflow:Calling model_fn.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /home/omar/tmp/tmphtt2bsg4/model.ckpt-1256\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           C       0.94      0.81      0.87     11974\n",
      "           M       0.35      0.68      0.47      1805\n",
      "\n",
      "   micro avg       0.80      0.80      0.80     13779\n",
      "   macro avg       0.65      0.75      0.67     13779\n",
      "weighted avg       0.87      0.80      0.82     13779\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.fit(X_train, y_train, num_epochs=5)\n",
    "model.score(X_test, y_test)\n",
    "print(classification_report(y_test, model.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
