{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from collections import Counter\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "from sklearn.metrics import f1_score, make_scorer, confusion_matrix, \\\n",
    "    classification_report\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, \\\n",
    "    StratifiedShuffleSplit, RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"../Data/Learn/sequences.pkl\", \"rb\") as f:\n",
    "    learn_sequences = pickle.load(f)\n",
    "    \n",
    "with open(\"../Data/Test/sequences.pkl\", \"rb\") as f:\n",
    "    test_sequences = pickle.load(f)\n",
    "    \n",
    "with open(\"../Data/dict.pkl\", \"rb\") as f:\n",
    "    vocabulary = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load fasttext embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embeddings_dict = KeyedVectors.load_word2vec_format(\"../Data/wiki.fr.vec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handle unknown words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28934 known words out of 30432\n"
     ]
    }
   ],
   "source": [
    "new_vocab_mapping, new_reverse_vocabulary = {}, {}\n",
    "\n",
    "# Known words\n",
    "i = -1\n",
    "for word in vocabulary:\n",
    "    if word in embeddings_dict:\n",
    "        i += 1\n",
    "        new_vocab_mapping[vocabulary[word]] = i\n",
    "        new_reverse_vocabulary[i] = word\n",
    "print(\"%d known words out of %d\" % (i + 1, len(vocabulary)))\n",
    "\n",
    "# Unknown words\n",
    "i += 1\n",
    "for word in vocabulary:\n",
    "    if word not in embeddings_dict:\n",
    "        new_vocab_mapping[vocabulary[word]] = i\n",
    "new_reverse_vocabulary[i] = \"<UNK>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30432 original words\n",
      "28935 new words (1 unknown)\n",
      "28934 max new words index\n"
     ]
    }
   ],
   "source": [
    "print(\"%d original words\" % len(new_vocab_mapping))\n",
    "print(\"%d new words (1 unknown)\" % len(np.unique(list(new_vocab_mapping.values()))))\n",
    "print(\"%d max new words index\" % max(new_vocab_mapping.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<UNK>'"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_new_words = 28935\n",
    "new_reverse_vocabulary[num_new_words - 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply new mappings to sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_learn_sequences = [list(map(new_vocab_mapping.get, seq)) for seq in learn_sequences]\n",
    "new_test_sequences = [list(map(new_vocab_mapping.get, seq)) for seq in test_sequences]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create corresponding embeddings array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embeddings = np.zeros((num_new_words, 300))\n",
    "# Last row is for unknown words, we leave it to zero for the moment\n",
    "for i in range(num_new_words - 1):\n",
    "    embeddings[i, :] = embeddings_dict[new_reverse_vocabulary[i]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\"J'aurai l'occasion de dire aux Français comment notre enseignement devra évoluer pour permettre à chaque jeune de trouver sa place, d'entrer dans le monde du travail, de savoir s'adapter et, à partir de là, d'acquérir, tout au long de la vie, de nouvelles compétences et de nouveaux savoirs.\",\n",
       " [28934, 2147, 28934, 309, 1],\n",
       " True)"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(learn_sentences[0], new_learn_sequences[0][:5], \n",
    "(embeddings[2147] == embeddings_dict.word_vec(\"aurai\")).all())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Et tous se demandaient : \" Après elle, après nous, qui se souviendra et comment ?',\n",
       " [4, 48, 36, 11372, 28934],\n",
       " True)"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(test_sentences[0], new_test_sequences[0][:5], \n",
    "(embeddings[48] == embeddings_dict.word_vec(\"tous\")).all())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Last row for unknown words\n",
    "embeddings[num_new_words - 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"../Data/generated/my_learn_sequences.pkl\", \"wb\") as f:\n",
    "    pickle.dump(new_learn_sequences, f)\n",
    "    \n",
    "with open(\"../Data/generated/my_test_sequences.pkl\", \"wb\") as f:\n",
    "    pickle.dump(new_test_sequences, f)\n",
    "    \n",
    "with open(\"../Data/generated/my_reverse_vocabulary.pkl\", \"wb\") as f:\n",
    "    pickle.dump(new_reverse_vocabulary, f)\n",
    "    \n",
    "with open(\"../Data/generated/my_embeddings.pkl\", \"wb\") as f:\n",
    "    pickle.dump(embeddings, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
