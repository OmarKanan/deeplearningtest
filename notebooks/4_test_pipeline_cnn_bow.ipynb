{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/omar/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.metrics import f1_score, make_scorer, confusion_matrix, \\\n",
    "    classification_report\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, \\\n",
    "    StratifiedShuffleSplit, RandomizedSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"../Data/Learn/labels.pkl\", \"rb\") as f:\n",
    "    learn_labels = pickle.load(f)\n",
    "\n",
    "with open(\"../Data/Learn/sequences.pkl\", \"rb\") as f:\n",
    "    learn_sequences = pickle.load(f)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    learn_sequences, learn_labels, test_size=0.3,\n",
    "    shuffle=True, stratify=learn_labels, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Test pipeline with bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DNNModel(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self,\n",
    "                 model_name=None,\n",
    "                 checkpoints_dir=\"../checkpoints/\",\n",
    "                 hidden_units=(10,),\n",
    "                 batch_size=128,\n",
    "                 weight_class_M=1.0,\n",
    "                 features_key=\"x\",\n",
    "                 weight_key=\"weight\"\n",
    "                 ):\n",
    "        self.set_model_directory(checkpoints_dir, model_name)\n",
    "        self.hidden_units = hidden_units\n",
    "        self.batch_size = batch_size\n",
    "        self.features_key = features_key\n",
    "        self.weight_key = weight_key\n",
    "        self.weight_class_M = weight_class_M\n",
    "\n",
    "    def fit(self, X, y, num_epochs=1, warm_start=True):\n",
    "        warm_start = self.check_warm_start(warm_start)\n",
    "        if not warm_start:\n",
    "            X, y = self.fit_and_apply_transformers(X, y)\n",
    "            self.classifier_ = self.create_dnn_classifier()\n",
    "        else:\n",
    "            X, y = self.apply_transformers(X, y)\n",
    "\n",
    "        self.classifier_.train(self.input_fn(\n",
    "            tf.estimator.ModeKeys.TRAIN, X, y, num_epochs))\n",
    "        return self\n",
    "\n",
    "    def fit_and_apply_transformers(self, X, y):\n",
    "        # Fit and transform X\n",
    "        self.vectorizer_ = CountVectorizer(lowercase=False,\n",
    "                                           tokenizer=self.tokens_to_str,\n",
    "                                           ngram_range=(1, 2))\n",
    "        X = self.vectorizer_.fit_transform(X)\n",
    "        self.n_features_ = len(self.vectorizer_.vocabulary_)\n",
    "\n",
    "        # Fit and transform y\n",
    "        self.label_encoder_ = LabelEncoder()\n",
    "        y = self.label_encoder_.fit_transform(y)\n",
    "        self.n_classes_ = len(self.label_encoder_.classes_)\n",
    "\n",
    "        return X, y\n",
    "\n",
    "    def apply_transformers(self, X, y):\n",
    "        X = self.vectorizer_.transform(X)\n",
    "        y = self.label_encoder_.transform(y)\n",
    "        return X, y\n",
    "\n",
    "    def predict(self, X):\n",
    "        X = self.vectorizer_.transform(X)\n",
    "        classes = list(self.classifier_.predict(self.input_fn(\n",
    "            tf.estimator.ModeKeys.PREDICT, X)))\n",
    "        labels = self.label_encoder_.inverse_transform(classes)\n",
    "        return labels\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        X, y = self.apply_transformers(X, y)\n",
    "        results = self.classifier_.evaluate(self.input_fn(\n",
    "            tf.estimator.ModeKeys.EVAL, X, y))\n",
    "        return results[\"f1-score\"]\n",
    "\n",
    "    def create_dnn_classifier(self):\n",
    "        # Columns of X\n",
    "        self.feature_columns_ = [tf.feature_column.numeric_column(\n",
    "            key=self.features_key, shape=self.n_features_\n",
    "        )]\n",
    "\n",
    "        # Column of weights which will be used to compute loss\n",
    "        weight_column = tf.feature_column.numeric_column(self.weight_key)\n",
    "        \n",
    "        # Model parameters\n",
    "        params = {\n",
    "            \"feature_columns\": self.feature_columns_,\n",
    "            \"weight_column\": weight_column,\n",
    "            \"hidden_units\": self.hidden_units,\n",
    "            \"n_classes\": self.n_classes_,\n",
    "        }\n",
    "        \n",
    "        # Create model\n",
    "        model = tf.estimator.Estimator(model_fn=self.model_fn,\n",
    "                                       model_dir=self.model_dir,\n",
    "                                       params=params)\n",
    "        model = tf.contrib.estimator.add_metrics(model, self.f1_score)\n",
    "        return model\n",
    "        \n",
    "    def model_fn(self, features, labels, mode, params):\n",
    "        # Network\n",
    "        logits = self.network_fn(features, params)\n",
    "        \n",
    "        # Predict\n",
    "        predicted_classes = tf.argmax(logits, 1)\n",
    "        if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "            return tf.estimator.EstimatorSpec(mode, predictions=predicted_classes)        \n",
    "        \n",
    "        # Loss\n",
    "        weights = tf.feature_column.input_layer(features, params['weight_column'])\n",
    "        loss = tf.losses.sparse_softmax_cross_entropy(labels=labels, \n",
    "                                                      logits=logits,\n",
    "                                                      weights=weights)\n",
    "\n",
    "        # Eval\n",
    "        if mode == tf.estimator.ModeKeys.EVAL:\n",
    "            return tf.estimator.EstimatorSpec(mode, loss=loss, predictions=predicted_classes)\n",
    "        \n",
    "        # Train\n",
    "        optimizer = tf.train.AdagradOptimizer(learning_rate=0.1)\n",
    "        train_op = optimizer.minimize(loss, global_step=tf.train.get_global_step())\n",
    "        return tf.estimator.EstimatorSpec(mode, loss=loss, train_op=train_op)\n",
    "    \n",
    "    def f1_score(self, labels, predictions):\n",
    "        return {\"f1-score\": self.f1_metric_fn(labels=labels, predictions=predictions)}\n",
    "    \n",
    "    def f1_metric_fn(self, labels, predictions):\n",
    "        p, p_op = tf.metrics.precision(labels=labels, predictions=predictions)\n",
    "        r, r_op = tf.metrics.recall(labels=labels, predictions=predictions)\n",
    "        return 2 * p * r / (p + r), tf.group(p_op, r_op)        \n",
    "\n",
    "    def input_fn(self, mode, X, y=None, num_epochs=1):\n",
    "        n = X.shape[0]\n",
    "        if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "            num_batches = n // self.batch_size\n",
    "        else:\n",
    "            num_batches = int(np.ceil(n / self.batch_size))\n",
    "\n",
    "        def mode_input_fn():\n",
    "            # Convert to sparse tensor\n",
    "            tf_X = self.convert_sparse_matrix_to_sparse_tensor(X)\n",
    "\n",
    "            # Batch iterator\n",
    "            i = tf.train.range_input_producer(\n",
    "                limit=num_batches, num_epochs=num_epochs, shuffle=False\n",
    "            ).dequeue()\n",
    "\n",
    "            # Slice sparse tensor using batch number, then convert to dense\n",
    "            tf_X = tf.sparse.to_dense(tf.sparse.slice(\n",
    "                tf_X, start=[i * self.batch_size, 0],\n",
    "                size=[self.batch_size, self.n_features_]\n",
    "            ), validate_indices=False)\n",
    "\n",
    "            if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "                return {self.features_key: tf_X}\n",
    "            \n",
    "            # Slice labels tensor using batch number\n",
    "            tf_y = tf.convert_to_tensor(y)\n",
    "            tf_y = tf_y[i * self.batch_size:(i + 1) * self.batch_size]\n",
    "\n",
    "            # Create weights tensor using given weight of class \"M\"\n",
    "            class_M = self.label_encoder_.transform([\"M\"])\n",
    "            weights = tf.cast(tf.equal(tf_y, class_M), tf.float64)\n",
    "            weights = tf.multiply(weights, (self.weight_class_M - 1)) + 1\n",
    "\n",
    "            # Return tensors in correct format\n",
    "            return {self.features_key: tf_X, self.weight_key: weights}, tf_y\n",
    "\n",
    "        return mode_input_fn\n",
    "\n",
    "    def check_warm_start(self, warm_start):\n",
    "        if warm_start:\n",
    "            # Check if model was already fitted\n",
    "            try:\n",
    "                self.classifier_\n",
    "            except:\n",
    "                warm_start = False\n",
    "        return warm_start\n",
    "\n",
    "    def set_model_directory(self, checkpoints_dir, model_name):\n",
    "        if model_name is not None:\n",
    "            self.model_dir = checkpoints_dir + model_name\n",
    "            # Check model_dir doesn't already exist\n",
    "            if os.path.exists(self.model_dir):\n",
    "                raise ValueError(\"model_dir already exists\")\n",
    "        else:\n",
    "            self.model_dir = None\n",
    "\n",
    "    @staticmethod\n",
    "    def tokens_to_str(tokens):\n",
    "        # Used in CountVectorizer because we already have tokens\n",
    "        return list(map(str, tokens))\n",
    "\n",
    "    @staticmethod\n",
    "    def convert_sparse_matrix_to_sparse_tensor(X):\n",
    "        coo = X.tocoo()\n",
    "        indices = np.mat([coo.row, coo.col]).transpose()\n",
    "        return tf.SparseTensorValue(indices, coo.data, coo.shape)\n",
    "    \n",
    "    def network_fn(self, features, params):\n",
    "        inputs = tf.feature_column.input_layer(features, params['feature_columns'])\n",
    "        inputs = inputs[:, :, tf.newaxis]\n",
    "        feature_maps = []\n",
    "        for filters, kernel_size in zip([5] * 2, [2, 3]):\n",
    "            tmp = tf.layers.conv1d(inputs, filters, kernel_size, padding=\"same\")\n",
    "            tmp = tf.layers.max_pooling1d(tmp, [self.n_features_], strides=1, padding=\"valid\")\n",
    "            feature_maps.append(tmp)\n",
    "        feature_maps = tf.reshape(tf.concat(feature_maps, axis=2), [-1, 5 * 2])\n",
    "        logits = tf.layers.dense(feature_maps, self.n_classes_, activation=None)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "WARNING:tensorflow:Using temporary folder as model directory: /home/omar/tmp/tmpggh347p3\n",
      "INFO:tensorflow:Using config: {'_model_dir': '/home/omar/tmp/tmpggh347p3', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f7278648da0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "INFO:tensorflow:Using config: {'_model_dir': '/home/omar/tmp/tmpggh347p3', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f7278656898>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 0 into /home/omar/tmp/tmpggh347p3/model.ckpt.\n",
      "INFO:tensorflow:loss = 1.3789406, step = 1\n",
      "INFO:tensorflow:Saving checkpoints for 3 into /home/omar/tmp/tmpggh347p3/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 1.3194814.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2018-11-16-14:24:49\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /home/omar/tmp/tmpggh347p3/model.ckpt-3\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2018-11-16-14:24:53\n",
      "INFO:tensorflow:Saving dict for global step 3: f1-score = 0.14568289, global_step = 3, loss = 1.2206159\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 3: /home/omar/tmp/tmpggh347p3/model.ckpt-3\n",
      "\n",
      "EPOCH 0: test f1-score = 0.146\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /home/omar/tmp/tmpggh347p3/model.ckpt-3\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 3 into /home/omar/tmp/tmpggh347p3/model.ckpt.\n",
      "INFO:tensorflow:loss = 1.0854783, step = 4\n",
      "INFO:tensorflow:Saving checkpoints for 6 into /home/omar/tmp/tmpggh347p3/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 1.2920526.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2018-11-16-14:24:55\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /home/omar/tmp/tmpggh347p3/model.ckpt-6\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2018-11-16-14:24:59\n",
      "INFO:tensorflow:Saving dict for global step 6: f1-score = 0.1941477, global_step = 6, loss = 1.2045436\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 6: /home/omar/tmp/tmpggh347p3/model.ckpt-6\n",
      "\n",
      "EPOCH 1: test f1-score = 0.194\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /home/omar/tmp/tmpggh347p3/model.ckpt-6\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 6 into /home/omar/tmp/tmpggh347p3/model.ckpt.\n",
      "INFO:tensorflow:loss = 1.099246, step = 7\n",
      "INFO:tensorflow:Saving checkpoints for 9 into /home/omar/tmp/tmpggh347p3/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 1.2659807.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2018-11-16-14:25:01\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /home/omar/tmp/tmpggh347p3/model.ckpt-9\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2018-11-16-14:25:05\n",
      "INFO:tensorflow:Saving dict for global step 9: f1-score = 0.22018677, global_step = 9, loss = 1.2003144\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 9: /home/omar/tmp/tmpggh347p3/model.ckpt-9\n",
      "\n",
      "EPOCH 2: test f1-score = 0.220\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /home/omar/tmp/tmpggh347p3/model.ckpt-9\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 9 into /home/omar/tmp/tmpggh347p3/model.ckpt.\n",
      "INFO:tensorflow:loss = 1.1165321, step = 10\n",
      "INFO:tensorflow:Saving checkpoints for 12 into /home/omar/tmp/tmpggh347p3/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 1.2582846.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2018-11-16-14:25:07\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /home/omar/tmp/tmpggh347p3/model.ckpt-12\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2018-11-16-14:25:11\n",
      "INFO:tensorflow:Saving dict for global step 12: f1-score = 0.22688276, global_step = 12, loss = 1.1993258\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 12: /home/omar/tmp/tmpggh347p3/model.ckpt-12\n",
      "\n",
      "EPOCH 3: test f1-score = 0.227\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /home/omar/tmp/tmpggh347p3/model.ckpt-12\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 12 into /home/omar/tmp/tmpggh347p3/model.ckpt.\n",
      "INFO:tensorflow:loss = 1.1198652, step = 13\n",
      "INFO:tensorflow:Saving checkpoints for 15 into /home/omar/tmp/tmpggh347p3/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 1.253691.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Calling model_fn.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2018-11-16-14:25:12\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /home/omar/tmp/tmpggh347p3/model.ckpt-15\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2018-11-16-14:25:17\n",
      "INFO:tensorflow:Saving dict for global step 15: f1-score = 0.22729744, global_step = 15, loss = 1.198664\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 15: /home/omar/tmp/tmpggh347p3/model.ckpt-15\n",
      "\n",
      "EPOCH 4: test f1-score = 0.227\n",
      "--------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "\n",
    "num_epochs = 5\n",
    "weight_class_M = Counter(y_train)[\"C\"] / Counter(y_train)[\"M\"]\n",
    "model = DNNModel(weight_class_M=weight_class_M, hidden_units=(10,))\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.fit(X_train[:500], y_train[:500], num_epochs=1, warm_start=True)\n",
    "    f1 = model.score(X_test, y_test)\n",
    "    print(\"\\nEPOCH %d: test f1-score = %.3f\" % (epoch, f1))\n",
    "    print(\"-\" * 80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
