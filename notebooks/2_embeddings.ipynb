{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.metrics import f1_score, make_scorer, confusion_matrix, \\\n",
    "    classification_report\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, \\\n",
    "    StratifiedShuffleSplit, RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"../Data/Learn/sentences.pkl\", \"rb\") as f:\n",
    "    learn_sentences = pickle.load(f)\n",
    "    \n",
    "with open(\"../Data/Test//sentences.pkl\", \"rb\") as f:\n",
    "    test_sentences = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1152449'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = []\n",
    "with(open(\"../Data/wiki.fr.vec\")) as f:\n",
    "    for line in f:\n",
    "        words.append(line.split()[0])\n",
    "words.pop(0)\n",
    "words_set = set(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(26709, 26709)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train\n",
    "my_prep_sentences = [re.sub(\"\\s*([^\\w\\s])\\s*\", \" \\\\1 \", x).strip().lower() for x in learn_sentences]\n",
    "my_prep_sentences = [re.sub(\"\\d+\", \" UNK \", x).strip().lower() for x in my_prep_sentences]\n",
    "my_vocab = list(set([x for sent in my_prep_sentences for x in sent.split()]))\n",
    "my_vocab = {x for x in my_vocab if x in words_set}\n",
    "my_vocab = {x: i + 1 for i, x in enumerate(my_vocab)}\n",
    "my_vocab[\"UNK\"] = 0\n",
    "my_reverse_vocab = {v: k for k, v in my_vocab.items()}\n",
    "my_sequences = [[my_vocab.get(x, 0) for x in sent.split()] for sent in my_prep_sentences]\n",
    "len(my_vocab), len(my_reverse_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Test\n",
    "my_prep_test_sentences = [re.sub(\"\\s*([^\\w\\s])\\s*\", \" \\\\1 \", x).strip().lower() for x in test_sentences]\n",
    "my_prep_test_sentences = [re.sub(\"\\d+\", \" UNK \", x).strip().lower() for x in my_prep_test_sentences]\n",
    "my_test_sequences = [[my_vocab.get(x, 0) for x in sent.split()] for sent in my_prep_test_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"j ' aurai l ' occasion de dire aux français comment notre enseignement devra évoluer pour permettre à chaque jeune de trouver sa place , d ' entrer dans le monde du travail , de savoir s ' adapter et , à partir de là , d ' acquérir , tout au long de la vie , de nouvelles compétences et de nouveaux savoirs .\",\n",
       " 'il est nécessaire .',\n",
       " \"dans votre coeur et dans votre vie , la confiance et l ' enthousiasme l ' emportent sur le doute .\",\n",
       " \"pour conduire ce débat dans un esprit de véritable dialogue , je compte , si nos partenaires en sont d ' accord , inviter au prochain sommet du g unk  , à lyon , pour une séance de travail , le secrétaire général des nations unies , le président de la banque mondiale et le directeur général du fonds monétaire international .\",\n",
       " \"la france et l ' europe construiront ainsi un avenir de coopération avec un proche - orient pacifié , stable , prospère , libéré des menaces de la guerre , de la prolifération , du terrorisme .\"]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_prep_sentences[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['et tous se demandaient :  \" après elle , après nous , qui se souviendra et comment ?',\n",
       " \"qui feront de l ' union ce grand espace de paix , de droits et de libertés , ce foyer de l ' esprit digne de son héritage , cette terre que nos citoyens aimeront habiter , cultiver , faire rayonner ensemble .\",\n",
       " \"l ' enfant a été , naturellement , le premier bénéficiaire des victoires remportées contre la maladie et contre la malnutrition .\",\n",
       " \"loin d ' être isolés , ces exemples se multiplient .\",\n",
       " \"vous l ' avez souligné , monsieur le président , les professionnels libéraux sont au service de l ' homme .\"]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_prep_test_sentences[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "embeddings = np.zeros((len(my_vocab), 301), dtype=np.float64)\n",
    "found_words = set()\n",
    "with(open(\"../Data/wiki.fr.vec\")) as f:\n",
    "    embeddings[0, -1] = 1\n",
    "    for line in f:\n",
    "        elements = line.split()\n",
    "        index = my_vocab.get(elements[0], None)\n",
    "        if index is None:\n",
    "            continue\n",
    "        found_words.add(elements[0])\n",
    "        embeddings[index, :300] = np.fromiter((float(x) for x in elements[-300:]), np.float64, 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(found_words) == len(my_vocab) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       " array([-5.2388e-01, -7.8864e-02, -2.1426e-01,  4.5050e-01, -1.6323e-01,\n",
       "        -3.9610e-01, -5.5822e-02,  8.8860e-02,  3.3263e-02,  3.5497e-01,\n",
       "        -3.7551e-01,  1.3466e-01, -3.9498e-02,  1.4558e-01,  3.1491e-01,\n",
       "        -5.3520e-01,  3.2063e-01, -9.7604e-02, -1.5786e-01,  1.7707e-01,\n",
       "        -2.1829e-02, -7.3604e-02,  8.8456e-02,  2.7271e-01, -4.0893e-01,\n",
       "        -2.8936e-01,  2.4357e-01, -1.6350e-01, -6.7114e-02,  1.4976e-01,\n",
       "         3.3309e-01, -4.3440e-01,  4.0727e-01,  3.3487e-01,  4.7274e-01,\n",
       "        -5.7360e-01,  2.0478e-01,  1.2135e-01,  5.1401e-01,  1.5250e-01,\n",
       "         3.2778e-01,  3.8101e-01,  1.1218e-01, -3.4354e-01,  1.9793e-01,\n",
       "        -2.4192e-01, -5.0641e-01,  1.2132e-01, -5.6554e-01,  2.7503e-01,\n",
       "        -2.3125e-01, -1.0025e-01, -6.2886e-01,  2.2202e-01, -4.7107e-01,\n",
       "        -3.1978e-01,  9.3475e-02, -2.6858e-01,  5.5748e-01,  1.8647e-01,\n",
       "        -2.4557e-01,  2.9630e-01,  3.3130e-01, -3.0762e-01,  2.1914e-01,\n",
       "        -2.4634e-01, -7.2260e-02,  4.7626e-02, -2.5871e-01, -2.2027e-01,\n",
       "         1.0663e-01, -1.1764e-02, -6.0280e-01, -1.8379e-01, -5.5807e-01,\n",
       "         2.4574e-01,  7.2814e-01,  3.3548e-02, -1.8053e-01, -1.0190e-01,\n",
       "         3.3781e-01, -4.4684e-02,  4.5331e-01, -2.8155e-01, -1.3315e-01,\n",
       "         2.6613e-01, -1.9714e-02,  1.1476e-01,  8.9919e-02, -4.1621e-01,\n",
       "        -7.6396e-02,  1.0953e-01,  8.2558e-02, -2.6993e-01, -3.6945e-01,\n",
       "         9.9119e-03,  1.9095e-01,  4.7297e-02, -1.6887e-01, -7.6009e-01,\n",
       "         9.6698e-02, -2.3095e-01, -3.5813e-01, -3.1443e-01,  1.0525e-01,\n",
       "         3.3958e-01, -3.4066e-01, -2.4725e-01,  3.6950e-02, -1.8910e-01,\n",
       "        -5.5334e-01,  2.2115e-01,  9.7390e-02,  1.1088e-01,  2.8406e-01,\n",
       "         6.1167e-02,  9.7744e-02,  2.1377e-01,  4.6907e-01,  5.8636e-02,\n",
       "        -4.0847e-01,  3.6540e-03, -2.9712e-01,  4.2349e-01, -1.3989e-01,\n",
       "        -2.7769e-01,  7.2513e-02, -6.1485e-02,  1.2001e-01,  9.1503e-02,\n",
       "         8.6311e-02, -2.4995e-01, -2.5109e-03, -1.6140e-01,  5.5310e-02,\n",
       "         6.8934e-02, -8.5304e-02,  3.4004e-02,  5.4109e-02,  4.2913e-01,\n",
       "        -7.9176e-02, -1.5438e-01, -2.6468e-01,  2.3164e-01, -2.0248e-01,\n",
       "         1.4857e-02,  2.2433e-01, -1.6947e-02, -2.6385e-01, -2.1714e-01,\n",
       "        -5.9074e-03, -2.8225e-01,  8.4788e-02, -1.0836e-01, -4.9276e-03,\n",
       "         3.0705e-01, -1.2914e-01,  2.1589e-01, -5.7316e-02,  4.9194e-01,\n",
       "         5.3811e-02,  2.6255e-01,  1.2905e-01,  3.0138e-01, -5.3062e-01,\n",
       "         1.1059e-01,  2.2559e-01, -1.3322e-01, -3.9034e-01, -3.9273e-01,\n",
       "        -8.1150e-02,  2.7493e-02,  1.6739e-01,  4.9709e-02, -1.3479e-01,\n",
       "        -3.3432e-02,  1.2864e-02, -1.8447e-01,  2.4752e-01, -2.1549e-01,\n",
       "         4.4933e-01, -1.9773e-01,  3.0671e-01,  5.1801e-01,  2.9163e-02,\n",
       "        -5.3504e-02, -4.5771e-02,  1.9800e-01,  3.8922e-01, -1.5555e-01,\n",
       "        -1.7729e-01,  6.8309e-01, -4.1315e-01,  9.4004e-02,  3.9898e-01,\n",
       "        -6.4628e-02,  2.7121e-01, -9.6155e-02,  3.1771e-02,  4.4655e-01,\n",
       "        -1.3788e-01,  2.6761e-01,  8.8063e-02, -1.1876e-01, -2.1994e-01,\n",
       "        -1.9952e-01,  8.6038e-02,  1.9493e-01,  1.8334e-01,  1.0687e-01,\n",
       "        -3.0046e-01,  5.7891e-02,  7.7596e-02, -4.2441e-01, -6.2909e-01,\n",
       "         4.4939e-01,  6.3029e-02, -4.8872e-02, -1.3904e-01,  2.1447e-01,\n",
       "         6.7633e-02, -9.6246e-02, -9.3072e-02, -1.8607e-01, -4.2188e-02,\n",
       "         2.6440e-01,  1.2000e-01, -5.4448e-01, -2.8473e-01,  4.3901e-01,\n",
       "        -5.9618e-01, -3.1016e-05, -3.0912e-01,  3.0871e-02, -1.0662e-01,\n",
       "         2.7614e-01,  2.9906e-01, -1.9866e-01, -1.2031e-01, -2.8591e-02,\n",
       "        -6.0770e-01, -4.1859e-02,  2.7239e-02,  5.6199e-01,  4.9102e-02,\n",
       "        -2.4908e-02, -1.1061e-01,  1.7801e-01,  5.4049e-01,  3.9013e-01,\n",
       "        -3.1213e-01,  1.6451e-01,  1.7826e-01, -4.9834e-01,  4.9404e-02,\n",
       "        -7.9801e-01, -1.9348e-02, -3.2016e-01,  2.4930e-01, -1.5650e-02,\n",
       "        -1.4523e-01, -2.6377e-01, -9.3329e-02,  4.4486e-01,  1.4970e-01,\n",
       "         8.2302e-02,  2.3325e-02, -6.9565e-02, -7.6811e-02, -1.7902e-02,\n",
       "         4.2150e-01,  8.5267e-02,  3.0624e-01,  2.4149e-01, -1.9775e-01,\n",
       "        -2.7076e-01,  1.4880e-03, -1.4924e-01, -1.0977e-01, -1.3519e-01,\n",
       "         2.8091e-02,  1.0483e-01, -7.4643e-02, -1.0069e-01, -1.6647e-01,\n",
       "        -8.5575e-02, -2.4414e-02, -1.6207e-02, -1.5115e-01, -8.6287e-02,\n",
       "        -2.6596e-02, -8.4646e-02,  4.2918e-01, -7.0033e-02, -4.0011e-02,\n",
       "         2.3098e-01,  4.2483e-01,  2.4757e-01,  1.5868e-01,  2.2880e-01,\n",
       "         0.0000e+00]))"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings[my_vocab[\"UNK\"]], embeddings[my_vocab[\"bonjour\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(\"../Data/generated\"):\n",
    "    os.mkdir(\"../Data/generated\")\n",
    "with open(\"../Data/generated/my_learn_sequences.p\", \"wb\") as f:\n",
    "    pickle.dump(my_sequences, f)\n",
    "with open(\"../Data/generated/my_test_sequences.p\", \"wb\") as f:\n",
    "    pickle.dump(my_test_sequences, f)\n",
    "with open(\"../Data/generated/my_vocab.p\", \"wb\") as f:\n",
    "    pickle.dump(my_vocab, f)\n",
    "with open(\"../Data/generated/my_embeddings.p\", \"wb\") as f:\n",
    "    pickle.dump(embeddings, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
